{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Vectorizaci√≥n de texto\n",
    "M√©todos:\n",
    "CountVectorizer: Representa el texto como una matriz de frecuencia de palabras.\n",
    "TfidfVectorizer: Pondera las palabras seg√∫n su relevancia en el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Vectorizaci√≥n\n",
    "\n",
    "## ¬øPor qu√© vectorizar?\n",
    "\n",
    "La vectorizaci√≥n es un proceso crucial en NLP ya que nos permite convertir material textual, desestructurado, en vectores num√©ricos. Dado que las computadoras no pueden procesar directamente texto en su forma original y que los modelos de Machine Learning trabajan con datos **num√©ricos y estructurados**, necesitamos traducir de cierta forma ese input a valores que las computadoras pueden procesar.\n",
    "\n",
    "### **Ejemplo:**\n",
    "Supongamos que queremos entrenar un modelo para detectar si una rese√±a de una pel√≠cula es positiva o negativa.\n",
    "\n",
    " **Entrada:**  \n",
    "*\"Me encant√≥ la pel√≠cula, la actuaci√≥n fue brillante.\"*\n",
    "\n",
    " **Lo que la computadora entiende:** \n",
    "\n",
    "üö´ `\"Me encant√≥ la pel√≠cula, la actuaci√≥n fue brillante.\"`  \n",
    "\n",
    "‚úÖ Vector num√©rico: `[0.2, 0.4, 0.7, 0.1, ...]`\n",
    "\n",
    "El texto necesita convertirse en **una representaci√≥n matem√°tica**, y el proceso de convertir palabras en n√∫meros se llama **vectorizaci√≥n**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Vectores:\n",
    "Los vectores son b√°sicamente conjuntos de n√∫meros que representan diversas caracter√≠sticas del texto. Estos conjuntos pueden tener distintas dimensiones:\n",
    "\n",
    "- Vectores 1D : representan palabras individuales (por ejemplo, word embeddings).\n",
    "- Vectores 2D : representan secuencias de palabras, como oraciones o documentos (por ejemplo, sentence embeddings).\n",
    "- Vectores multidimensionales : pueden representar estructuras y relaciones m√°s complejas, involucrando potencialmente espacios de dimensiones superiores.\n",
    "\n",
    "Al aplicar diferentes t√©cnicas de vectorizaci√≥n, los vectores resultantes variar√°n seg√∫n el m√©todo utilizado. Cada t√©cnica produce vectores con caracter√≠sticas y rangos de valores √∫nicos. \n",
    "\n",
    "Por ejemplo, algunas t√©cnicas producen valores binarios (0 o 1), mientras que otras producen valores continuos entre 0 y 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En Python podemos pensar que un vector es una lista de n√∫meros\n",
    "# TODO: Verificar que sea necesario aclarar esto, a esta altura del partido es confundirlos aun mas... \n",
    "import numpy as np\n",
    "\n",
    "vector = [1, 2, 3, 4, 5]\n",
    "otro_vector = [[0.1, 0.2, 0.8, -0.3, 0.5]]\n",
    "\n",
    "np_vector = np.array(vector)\n",
    "np_otro_vector = np.array(otro_vector)\n",
    "\n",
    "\n",
    "print(np_vector)\n",
    "print(np_vector.shape) # (5,) -> 5 elementos en una dimensi√≥n (1D vector)\n",
    "\n",
    "print(np_otro_vector)\n",
    "print(np_otro_vector.shape) # (1, 5) -> 1 fila con 5 elementos (2D vector). Ser√≠a una matriz de 1x5 en lugar de un vector de 5 elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìå **El desaf√≠o de representar el texto**\n",
    "Al vectorizar texto, buscamos representar su significado sem√°ntico en forma num√©rica. Esto implica preservar al m√°ximo los matices y el contexto de las palabras, permitiendo que los modelos los interpreten y utilicen de manera efectiva.\n",
    "\n",
    "\n",
    "| Caracter√≠stica | Desaf√≠o |\n",
    "|--------------|---------|\n",
    "| El lenguaje es ambiguo | \"Banco\" puede referirse a una entidad financiera o un asiento. |\n",
    "| Orden de palabras importa | \"El perro mordi√≥ al hombre\" vs. \"El hombre mordi√≥ al perro\". |\n",
    "| Las palabras tienen significado | \"Perro\" y \"can\" son similares, pero el modelo debe aprender esto. |\n",
    "| Vocabulario extenso | ¬øC√≥mo representar millones de palabras en un espacio finito? |\n",
    "\n",
    "\n",
    "\n",
    "üìç Ejemplo : Imaginemos que tenemos dos oraciones:\n",
    "\n",
    "- \"El perro se sienta en la alfombra\".\n",
    "- \"Un can descansa sobre tapete\".\n",
    "\n",
    "\n",
    "Sin la vectorizaci√≥n, un modelo de ML ver√≠a estos strings de formas completamente diferentes porque utilizan palabras diferentes. Pero con vectorizaci√≥n, especialmente con t√©cnicas avanzadas como *word embeddings*, el modelo puede entender que \"perro\" y \"can\", as√≠ como \"alfombra\" y \"tapete\", tienen significados similares.\n",
    "\n",
    "Supongamos que usamos una t√©cnica de vectorizaci√≥n avanzada que nos permite hacer lo siguiente:\n",
    "\n",
    "- \"perro\" podr√≠a convertirse en un vector como `[0,2, 0,5, 0,1, ‚Ä¶]`\n",
    "- \"can\" podr√≠a convertirse en un vector como `[0,21, 0,49, 0,12, ‚Ä¶]`\n",
    "\n",
    "\n",
    "En este caso, los vectores de \"perro\" y \"can\" estar√≠an muy pr√≥ximos en el espacio vectorial, lo que indica su similitud sem√°ntica. Lo mismo ocurre con \"tapete\" y \"alfombra\".\n",
    "\n",
    "\n",
    "Esto significa que debemos elegir **formas de vectorizaci√≥n** que conserven **informaci√≥n clave** sin hacer el problema computacionalmente inmanejable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Distintos m√©todos de vectorizaci√≥n**\n",
    "Existen distintos m√©todos de vectorizaci√≥n, que van desde **t√©cnicas simples** hasta **modelos m√°s avanzados**, con sus complejidades particulares:\n",
    "\n",
    "### üîπ Representaciones b√°sicas:\n",
    "1. **One-Hot Encoding:** \n",
    "- Representacion binaria, indica precencia o ausencia de una palabra.\n",
    "- Suele usarse en contextos generales sobre variables categ√≥ricas.\n",
    "- Escala muy mal > Se genera un vector de dimensi√≥n igual al n√∫mero de palabras. \n",
    "\n",
    "2. **Bag of Words:** \n",
    "- Representa frecuencia de palabras.\n",
    "- A diferencia del OHE, no s√≥lo indica si la palabra est√° presente sino cu√°ntas veces aparece.\n",
    "- No considera la posici√≥n de las palabras ni el contexto.\n",
    "- No tiene en cuenta la sem√°ntica\n",
    "\n",
    "3. **TF-IDF (Term Frequency - Inverse Document Frequency):** \n",
    "- Parte de la idea de BOW pero ajusta la importancia de palabras comunes en el corpus.\n",
    "- Reduce el peso de las palabras muy frecuentes (como stopwords), a la vez que aumenta la importancia de las menos frecuentes (distintivas para un documento).\n",
    "\n",
    "### üîπ Representaciones m√°s avanzadas:\n",
    "4. **Word Embeddings (Word2Vec, GloVe, FastText):** \n",
    "- Capturan la sem√°ntica de las palabras y sus relaciones en un espacio vectorial de menor dimensi√≥n.\n",
    "- Permiten capturar similitudes entre palabras a partir de su distancia/cercan√≠a en el espacio vectorial.\n",
    "\n",
    "5. **Representaciones Contextuales (BERT, GPT):** \n",
    "- Extienden la capacidad de los _embeddings_ teniendo en cuenta el contexto completo en el que aparece una palabra.\n",
    "- Cada palabra el documento tiene una representacion distinta seg√∫n las palabras que la rodean. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Para esta clase, nos enfocaremos en **las primeras tres t√©cnicas** y c√≥mo usarlas en un **Na√Øve Bayes Classifier** para clasificaci√≥n de texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Bag of Words\n",
    "Para preparar el terreno, vamos a ver un ejemplo sencillo de c√≥mo un texto puede representarse num√©ricamente.\n",
    "\n",
    "**Texto original:**\n",
    "- \"El gato duerme en la cama\"\n",
    "- \"El perro ladra en la calle\"\n",
    "\n",
    " **Vocabulario √∫nico (Bag of Words):**  \n",
    "\n",
    "- [\"el\", \"gato\", \"duerme\", \"en\", \"la\", \"cama\", \"perro\", \"ladra\", \"calle\"]\n",
    "\n",
    " **Matriz de conteo:**\n",
    "\n",
    "| Documento | el | gato | duerme | en | la | cama | perro | ladra | calle |\n",
    "|-----------|----|------|--------|----|----|------|-------|-------|-------|\n",
    "| \"El gato duerme en la cama\" | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 |\n",
    "| \"El perro ladra en la calle\" | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 1 |\n",
    "\n",
    "Cada fila representa un documento y cada columna una palabra del vocabulario. \n",
    "\n",
    "**Esto es lo que los modelos de Machine Learning ven** en lugar del texto original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from funciones import preprocess_text\n",
    "\n",
    "\n",
    "corpus = [\"El gato duerme en la cama\", \"El perro ladra en la calle\", \"Un gato descansa en el tapete\", \"Los perros juegan en la calle\", \"El gato y el perro juegan.\"]\n",
    "norm_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(norm_corpus)\n",
    "print(\"Matriz Termino documento\")\n",
    "print(vectorizer.get_feature_names_out())  # Ver palabras en vocabulario\n",
    "print(X.toarray())  # Matriz de t√©rminos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones import plot_text_word_matrix\n",
    "\n",
    "\n",
    "plot_text_word_matrix(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "words = [token for token in norm_corpus]\n",
    "\n",
    "FreqDist(norm_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**üîπ ¬øQu√© problema tiene este enfoque?**\n",
    "- No considera el orden de las palabras.\n",
    "- No diferencia palabras con significados similares.\n",
    "- Puede generar vectores muy grandes y dispersos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones import plot_bag_of_words\n",
    "\n",
    "plot_bag_of_words(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "One of the earliest methods of text representation that laid the groundwork for subsequent advances was Term Frequency‚ÄìInverse Document Frequency (TF‚ÄìIDF). This method quantifies the importance of a word within a document relative to a corpus by accounting for term frequency and inverse document frequency\n",
    "\n",
    "Uno de los primeros m√©todos de representaci√≥n textual que sent√≥ las bases para los siguientes avances es TF-IDF, por sus siglas en ing√©s. Esta t√©cnica 'cuantifica' la importancia de una palabra en un documento en relaci√≥n con un corpus teniendo en cuenta la **frecuencia de t√©rminos y la frecuencia inversa de documentos.\n",
    "\n",
    "Punto clave: Ajusta la frecuencia de las palabras para evitar que t√©rminos comunes dominen la representaci√≥n.\n",
    "\n",
    "Conceptos clave:\n",
    "\n",
    "- TF (Term Frequency): Frecuencia normalizada de una palabra en un documento.\n",
    "- IDF (Inverse Document Frequency): Reduce la importancia de palabras muy frecuentes en todos los documentos.\n",
    "\n",
    "\n",
    "üîπ Ventaja: Mejora la representaci√≥n considerando la relevancia de palabras. \n",
    "\n",
    "\n",
    "üîπ Desventaja: No captura la sem√°ntica de las palabras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#corpus = [\"El gato duerme.\", \"El perro ladra.\", \"El gato y el perro juegan.\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(norm_corpus)\n",
    "print(\"Matriz de frecuencia de palabras:\")\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones import plot_tfif_matrix\n",
    "\n",
    "plot_tfif_matrix(X, norm_corpus, vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicacion de vectorizaci√≥n en un clasificador\n",
    "Objetivo: Demostrar c√≥mo las t√©cnicas de vectorizaci√≥n pueden alimentar un modelo de Machine Learning real para tareas de NLP, en este caso, clasificaci√≥n de texto...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "csv_data = pd.read_csv(\"synth_data_sentiment.csv\")\n",
    "\n",
    "\n",
    "csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentiment_corpus = csv_data[\"text\"].to_list()[:500]\n",
    "sentiment_labels = csv_data[\"label\"].tolist()[:500]  # 1 = positivo, 0 = negativo\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentiment_corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())  # Ver palabras en vocabulario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bag_of_words(sentiment_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones import plot_text_word_matrix\n",
    "\n",
    "plot_text_word_matrix(list(set(corpus))) ## TODO: Sortear por frwequencia de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicaci√≥n:\n",
    "\n",
    "Cada oraci√≥n se transforma en un vector basado en la frecuencia de palabras.\n",
    "Se observa que las palabras m√°s comunes tendr√°n valores m√°s altos.\n",
    "\n",
    "\n",
    "Luego usamos Na√Øve Bayes,  un modelo basado en probabilidad, ideal para clasificaci√≥n de texto porque asume independencia entre palabras.\n",
    "Al usar Count Vectorizer, cada documento se representa como una bolsa de palabras, lo que encaja bien con la suposici√≥n del modelo.\n",
    "\n",
    "# TODO: ver de plotear palabras y ocurrencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluamos el modelo\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nueva_sentence = \"Wow‚Ä¶ Me encant√≥ esperar dos horas para que me trajeran la comida fr√≠a.\"\n",
    "# print(model.predict(vectorizer.transform([nueva_sentence])))<\n",
    "\n",
    "if model.predict(vectorizer.transform([nueva_sentence])) == 1:\n",
    "    print(\"Sentimiento positivo\")\n",
    "else: \n",
    "    print(\"Sentimiento negativo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar como usando IFIDF se puede mejorar el modelo\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from funciones import preprocess_text\n",
    "\n",
    "norm_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy con TF-IDF: {accuracy_score(y_test, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(vectorizer.transform([nueva_sentence])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(texts):\n",
    "    # Vectorize the text\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a dummy true labels and predicted labels for demonstration\n",
    "    true_labels = np.random.choice(words, len(words))\n",
    "    predicted_labels = np.random.choice(words, len(words))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels, labels=words)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xticks(np.arange(len(words)))\n",
    "    ax.set_yticks(np.arange(len(words)))\n",
    "    ax.set_xticklabels(sorted(words), rotation=90)\n",
    "    ax.set_yticklabels(sorted(words))\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix of Words')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_text_word_matrix(texts):\n",
    "    # Vectorize the text\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Convert the sparse matrix to a dense matrix\n",
    "    X_dense = X.toarray()\n",
    "\n",
    "    # Plot the matrix\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    cax = ax.matshow(X_dense, cmap=plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xticks(np.arange(len(words)))\n",
    "    ax.set_yticks(np.arange(len(texts)))\n",
    "    ax.set_xticklabels(words, rotation=90)\n",
    "    ax.set_yticklabels([f'{i}' for i in texts])\n",
    "\n",
    "    plt.xlabel('Vocabulario')\n",
    "    plt.ylabel('Textos')\n",
    "    plt.title('Matriz de Frecuencia de Palabras')\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso\n",
    "texts = [\n",
    "    \"Este es un texto de ejemplo\",\n",
    "    \"Otro texto de ejemplo para vectorizaci√≥n\",\n",
    "    \"Ejemplo de vectorizaci√≥n de texto\"\n",
    "]\n",
    "plot_text_word_matrix(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, [1, 0, 1], test_size=0.2)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Precisi√≥n del modelo:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
