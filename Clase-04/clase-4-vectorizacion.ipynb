{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sugerencias de uso de la Notebook: \n",
    "- Sugerimos 'Abrir en Colab' y realizar una copia del cuaderno antes de usarlo.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seminario-algosups/seminario-algosups.github.io/blob/master/Clase-04/clase-4-vectorizacion.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para obtener el archivo de la librer√≠a que usaremos en la clase, descomentar la siguiente l√≠nea. \n",
    "# Si una no funciona, probar la otra.:\n",
    "# !wget https://raw.githubusercontent.com/seminario-algosups/seminario-algosups.github.io/master/Clase-04/funciones.py\n",
    "# !wget https://github.com/seminario-algosups/seminario-algosups.github.io/blob/master/Clase-04/funciones.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üéØ **Objetivo de la clase:**\n",
    "Al finalizar esta clase podr√°s:\n",
    "- Transformar textos en vectores num√©ricos usando t√©cnicas como One-Hot Encoding, Bag of Words y TF-IDF.\n",
    "- Comprender c√≥mo la representaci√≥n vectorial influye en los modelos de clasificaci√≥n supervisada como Na√Øve Bayes.\n",
    "- Aplicar estas t√©cnicas para resolver problemas pr√°cticos de clasificaci√≥n de texto.\n",
    "\n",
    "### Pero, pero, pero...\n",
    "\n",
    "- Retormar conceptos de la clase anterior\n",
    "\n",
    "# M√©tricas de Evaluaci√≥n para Modelos de Clasificaci√≥n\n",
    "\n",
    "- M√©tricas fundamentales para evaluar el rendimiento de un modelo de clasificaci√≥n binaria.\n",
    "\n",
    "## Precision\n",
    "\n",
    "La precisi√≥n indica qu√© proporci√≥n de las predicciones positivas del modelo fueron realmente positivas.\n",
    "\n",
    "$$\n",
    "\\text{Precisi√≥n} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- \\(TP\\): Verdaderos Positivos\n",
    "- \\(FP\\): Falsos Positivos\n",
    "\n",
    "\n",
    "\n",
    "####  Recall (Cobertura)\n",
    "\n",
    "La recall mide la capacidad del modelo para identificar correctamente todas las instancias positivas.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- \\(FN\\): Falsos Negativos\n",
    "\n",
    "\n",
    "\n",
    "#### Accuracy (Exactitud)\n",
    "\n",
    "La Accuracy eval√∫a la proporci√≥n de predicciones correctas (positivas y negativas) sobre el total.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- \\(TN\\): Verdaderos Negativos\n",
    "\n",
    "\n",
    "\n",
    "####  F1 Score\n",
    "\n",
    "El F1 Score es la media arm√≥nica entre la precisi√≥n y el recall. Es especialmente √∫til cuando los datos est√°n desbalanceados.\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\cdot \\frac{\\text{Precisi√≥n} \\cdot \\text{Recall}}{\\text{Precisi√≥n} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Vectorizaci√≥n\n",
    "\n",
    "## ¬øPor qu√© vectorizar?\n",
    "\n",
    "La vectorizaci√≥n es un proceso crucial en NLP ya que nos permite convertir material textual, desestructurado, en vectores num√©ricos. Dado que las computadoras no pueden procesar directamente texto en su forma original y que los modelos de Machine Learning trabajan con datos **num√©ricos y estructurados**, necesitamos traducir de cierta forma ese input a valores que las computadoras pueden procesar.\n",
    "\n",
    "\n",
    "\n",
    "### **Ejemplo:**\n",
    "Supongamos que queremos entrenar un modelo para detectar si una rese√±a de una pel√≠cula es positiva o negativa.\n",
    "\n",
    " **Entrada:**  \n",
    "*\"Me encant√≥ la pel√≠cula, la actuaci√≥n fue brillante.\"*\n",
    "\n",
    " **Lo que la computadora entiende:** \n",
    "\n",
    "üö´ `\"Me encant√≥ la pel√≠cula, la actuaci√≥n fue brillante.\"`  \n",
    "\n",
    "‚úÖ Vector num√©rico: `[0.2, 0.4, 0.7, 0.1, ...]`\n",
    "\n",
    "El texto necesita convertirse en **una representaci√≥n matem√°tica**, y el proceso de convertir palabras en n√∫meros se llama **vectorizaci√≥n**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Vectores:\n",
    "Los vectores son b√°sicamente conjuntos de n√∫meros que representan diversas caracter√≠sticas del texto. Estos conjuntos pueden tener distintas dimensiones:\n",
    "\n",
    "- Vectores 1D : representan palabras individuales (por ejemplo, word embeddings).\n",
    "- Vectores 2D : representan secuencias de palabras, como oraciones o documentos (por ejemplo, sentence embeddings).\n",
    "- Vectores multidimensionales : pueden representar estructuras y relaciones m√°s complejas, involucrando potencialmente espacios de dimensiones superiores.\n",
    "\n",
    "Al aplicar diferentes t√©cnicas de vectorizaci√≥n, los vectores resultantes variar√°n seg√∫n el m√©todo utilizado. Cada t√©cnica produce vectores con caracter√≠sticas y rangos de valores √∫nicos. \n",
    "\n",
    "Por ejemplo, algunas t√©cnicas producen valores binarios (0 o 1), mientras que otras producen valores continuos entre 0 y 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En Python podemos pensar que un vector es una lista de n√∫meros\n",
    "# TODO: Verificar que sea necesario aclarar esto, a esta altura del partido es confundirlos aun mas... \n",
    "import numpy as np\n",
    "\n",
    "vector = [1, 2, 3, 4, 5]\n",
    "otro_vector = [[0.1, 0.2, 0.8, -0.3, 0.5]]\n",
    "\n",
    "np_vector = np.array(vector)\n",
    "np_otro_vector = np.array(otro_vector)\n",
    "\n",
    "\n",
    "print(np_vector)\n",
    "print(np_vector.shape) # (5,) -> 5 elementos en una dimensi√≥n (1D vector)\n",
    "\n",
    "print(np_otro_vector)\n",
    "print(np_otro_vector.shape) # (1, 5) -> 1 fila con 5 elementos (2D vector). Ser√≠a una matriz de 1x5 en lugar de un vector de 5 elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìå **El desaf√≠o de representar el texto**\n",
    "Al vectorizar texto, buscamos representar su significado sem√°ntico en forma num√©rica. Esto implica preservar al m√°ximo los matices y el contexto de las palabras, permitiendo que los modelos los interpreten y utilicen de manera efectiva.\n",
    "\n",
    "\n",
    "| Caracter√≠stica | Desaf√≠o |\n",
    "|--------------|---------|\n",
    "| El lenguaje es ambiguo | \"Banco\" puede referirse a una entidad financiera o un asiento. |\n",
    "| Orden de palabras importa | \"El perro mordi√≥ al hombre\" vs. \"El hombre mordi√≥ al perro\". |\n",
    "| Las palabras tienen significado | \"Perro\" y \"can\" son similares, pero el modelo debe aprender esto. |\n",
    "| Vocabulario extenso | ¬øC√≥mo representar millones de palabras en un espacio finito? |\n",
    "\n",
    "\n",
    "\n",
    "üìç Ejemplo : Imaginemos que tenemos dos oraciones:\n",
    "\n",
    "- \"El perro se sienta en la alfombra\".\n",
    "- \"Un can descansa sobre tapete\".\n",
    "\n",
    "\n",
    "Sin la vectorizaci√≥n, un modelo de ML ver√≠a estos strings de formas completamente diferentes porque utilizan palabras diferentes. Pero con vectorizaci√≥n, especialmente con t√©cnicas avanzadas como *word embeddings*, el modelo puede entender que \"perro\" y \"can\", as√≠ como \"alfombra\" y \"tapete\", tienen significados similares.\n",
    "\n",
    "Supongamos que usamos una t√©cnica de vectorizaci√≥n avanzada que nos permite hacer lo siguiente:\n",
    "\n",
    "- \"perro\" podr√≠a convertirse en un vector como `[0,2, 0,5, 0,1, ‚Ä¶]`\n",
    "- \"can\" podr√≠a convertirse en un vector como `[0,21, 0,49, 0,12, ‚Ä¶]`\n",
    "\n",
    "\n",
    "En este caso, los vectores de \"perro\" y \"can\" estar√≠an muy pr√≥ximos en el espacio vectorial, lo que indica su similitud sem√°ntica. Lo mismo ocurre con \"tapete\" y \"alfombra\".\n",
    "\n",
    "\n",
    "Esto significa que debemos elegir **formas de vectorizaci√≥n** que conserven **informaci√≥n clave** sin hacer el problema computacionalmente inmanejable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Distintos m√©todos de vectorizaci√≥n**\n",
    "Existen distintos m√©todos de vectorizaci√≥n, que van desde **t√©cnicas simples** hasta **modelos m√°s avanzados**, con sus complejidades particulares:\n",
    "\n",
    "### üîπ Representaciones b√°sicas:\n",
    "1. **One-Hot Encoding:** \n",
    "- Representacion binaria, indica precencia o ausencia de una palabra.\n",
    "- Suele usarse en contextos generales sobre variables categ√≥ricas.\n",
    "- Escala muy mal > Se genera un vector de dimensi√≥n igual al n√∫mero de palabras. \n",
    "\n",
    "2. **Bag of Words:** \n",
    "- Representa frecuencia de palabras.\n",
    "- A diferencia del OHE, no s√≥lo indica si la palabra est√° presente sino cu√°ntas veces aparece.\n",
    "- No considera la posici√≥n de las palabras ni el contexto.\n",
    "- No tiene en cuenta la sem√°ntica\n",
    "\n",
    "3. **TF-IDF (Term Frequency - Inverse Document Frequency):** \n",
    "- Parte de la idea de BOW pero ajusta la importancia de palabras comunes en el corpus.\n",
    "- Reduce el peso de las palabras muy frecuentes (como stopwords), a la vez que aumenta la importancia de las menos frecuentes (distintivas para un documento).\n",
    "\n",
    "### üîπ Representaciones m√°s avanzadas:\n",
    "4. **Word Embeddings (Word2Vec, GloVe, FastText):** \n",
    "- Capturan la sem√°ntica de las palabras y sus relaciones en un espacio vectorial de menor dimensi√≥n.\n",
    "- Permiten capturar similitudes entre palabras a partir de su distancia/cercan√≠a en el espacio vectorial.\n",
    "\n",
    "5. **Representaciones Contextuales (BERT, GPT):** \n",
    "- Extienden la capacidad de los _embeddings_ teniendo en cuenta el contexto completo en el que aparece una palabra.\n",
    "- Cada palabra el documento tiene una representacion distinta seg√∫n las palabras que la rodean. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Para esta clase, nos enfocaremos en **las primeras tres t√©cnicas** y c√≥mo usarlas en un **Na√Øve Bayes Classifier** para clasificaci√≥n de texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Bag of Words\n",
    "Para preparar el terreno, vamos a ver un ejemplo sencillo de c√≥mo un texto puede representarse num√©ricamente.\n",
    "\n",
    "**Texto original:**\n",
    "- \"El gato duerme en la cama\"\n",
    "- \"El perro ladra en la calle\"\n",
    "\n",
    " **Vocabulario √∫nico (Bag of Words):**  \n",
    "\n",
    "- [\"el\", \"gato\", \"duerme\", \"en\", \"la\", \"cama\", \"perro\", \"ladra\", \"calle\"]\n",
    "\n",
    " **Matriz de conteo:**\n",
    "\n",
    "| Documento | el | gato | duerme | en | la | cama | perro | ladra | calle |\n",
    "|-----------|----|------|--------|----|----|------|-------|-------|-------|\n",
    "| \"El gato duerme en la cama\" | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 |\n",
    "| \"El perro ladra en la calle\" | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 1 |\n",
    "\n",
    "Cada fila representa un documento y cada columna una palabra del vocabulario. \n",
    "\n",
    "**Esto es lo que los modelos de Machine Learning ven** en lugar del texto original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy scikit-learn matplotlib nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text, language='spanish')\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = [\"El gato duerme en la cama\", \"El perro ladra en la calle\", \"Un gato descansa en el tapete\", \"Los perros juegan en la calle\", \"El gato y el perro juegan.\"]\n",
    "norm_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "\n",
    "tokens = [word for doc in norm_corpus for word in word_tokenize(doc)]\n",
    "fdist = FreqDist(tokens)\n",
    "print(fdist.most_common()) # Pasarle n como argumento para limitar la cantidad de palabras a mostrar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv_vectorizer = CountVectorizer()\n",
    "X_cv_vectorizer = cv_vectorizer.fit_transform(norm_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matriz Termino documento\")\n",
    "print(cv_vectorizer.get_feature_names_out())  # Ver palabras en vocabulario\n",
    "print(\"\\n\", X_cv_vectorizer.toarray())  # Matriz de t√©rminos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones import plot_text_word_matrix\n",
    "\n",
    "plot_text_word_matrix(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones import plot_bag_of_words, plot_tfidf_matrix, plot_text_word_matrix\n",
    "\n",
    "#plot_bag_of_words(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**üîπ ¬øQu√© problema tiene este enfoque?**\n",
    "- No considera el orden de las palabras.\n",
    "- No diferencia palabras con significados similares.\n",
    "- Puede generar vectores muy grandes y dispersos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "*One of the earliest methods of text representation that laid the groundwork for subsequent advances was Term Frequency‚ÄìInverse Document Frequency (TF‚ÄìIDF). This method quantifies the importance of a word within a document relative to a corpus by accounting for term frequency and inverse document frequency*\n",
    "\n",
    "Uno de los primeros m√©todos de representaci√≥n textual que sent√≥ las bases para los siguientes avances es TF-IDF, por sus siglas en ing√©s. Esta t√©cnica 'cuantifica' la importancia de una palabra en un documento en relaci√≥n con un corpus teniendo en cuenta la **frecuencia de t√©rminos y la frecuencia inversa de documentos.\n",
    "\n",
    "Punto clave: Ajusta la frecuencia de las palabras para evitar que t√©rminos comunes dominen la representaci√≥n.\n",
    "\n",
    "Conceptos clave:\n",
    "\n",
    "- TF (Term Frequency): Frecuencia normalizada de una palabra en un documento.\n",
    "- IDF (Inverse Document Frequency): Reduce la importancia de palabras muy frecuentes en todos los documentos.\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{N√∫mero de veces que } t \\text{ aparece en } d}{\\text{N√∫mero total de t√©rminos en } d}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log \\left( \\frac{N}{n_t} \\right)\n",
    "$$\n",
    "\n",
    "üîπ Ventaja: Mejora la representaci√≥n considerando la relevancia de palabras. \n",
    "\n",
    "\n",
    "üîπ Desventaja: No captura la sem√°ntica de las palabras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(norm_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matriz de TF-IDF:\")\n",
    "print(X_tfidf.toarray())\n",
    "print(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones import plot_tfidf_matrix\n",
    "\n",
    "plot_tfidf_matrix(X_tfidf, norm_corpus, tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicacion de vectorizaci√≥n en un clasificador\n",
    "\n",
    "- Aplicar t√©cnicas b√°sicas de limpieza y normalizaci√≥n de datos.\n",
    "- Aplicar t√©cnicas b√°sicas de vectorizaci√≥n sobre los datos.\n",
    "- Alimentar un modelo de ML para una tarea espec√≠fica de NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "csv_data = pd.read_csv(\"synth_data_sentiment.csv\")\n",
    "\n",
    "#csv_data.head()\n",
    "#csv_data.info()\n",
    "#len(cvs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limpieza de los datos\n",
    "\n",
    "# Eliminar filas con valores nulos\n",
    "csv_data = csv_data.dropna()\n",
    "# Eliminar filas duplicadas\n",
    "csv_data = csv_data.drop_duplicates()\n",
    "# Eliminar filas con texto vac√≠o\n",
    "csv_data = csv_data[csv_data['text'].str.strip() != '']\n",
    "\n",
    "# Normalizar nuestros documentos\n",
    "#csv_data['norm_text'] = csv_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_corpus = csv_data[\"text\"].to_list() #  \n",
    "sentiment_labels = csv_data[\"label\"].tolist()  # 1 = positivo, 0 = negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos comenzar x revisar la nube de palabras, o directamente la matriz. \n",
    "# Matriz dispersa\n",
    "plot_bag_of_words(sentiment_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funciones import plot_text_word_matrix, plot_tfidf_matrix\n",
    "\n",
    "# plot_bag_of_words(sentiment_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def vectorize_corpus(vectorizer, corpus, preprocess=True):\n",
    "    \"\"\"\n",
    "    Vectoriza un corpus de texto utilizando el vectorizer requerido.\n",
    "    \"\"\"\n",
    "    if vectorizer == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(stop_words=list(stop_words))\n",
    "    elif vectorizer == 'count': \n",
    "        vectorizer = CountVectorizer()\n",
    "    else: \n",
    "        raise ValueError(\"El vectorizer debe ser 'tfidf' o 'count'\")\n",
    "    \n",
    "    # Si el corpus es una lista de documentos, vectorizamos cada uno\n",
    "    # Si el corpus es un solo documento, lo vectorizamos directamente\n",
    "    if isinstance(corpus, list):\n",
    "        corpus = [preprocess_text(doc) for doc in corpus] if preprocess else [doc for doc in corpus]\n",
    "    else:\n",
    "        corpus = preprocess_text(corpus) if preprocess else corpus   \n",
    "    \n",
    "    # Vectorizamos el corpus\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    return X, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Utilicemos countVectorizer como ejemplo\n",
    "\n",
    "X_count, count_vectorizer = vectorize_corpus('count', sentiment_corpus, preprocess=False)\n",
    "print(\"Matriz de conteo:\")\n",
    "print(X_count.toarray()[0])\n",
    "print(\"\\nVocabulario (50):\") \n",
    "print(count_vectorizer.get_feature_names_out()[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Ahora utilizamos tfidfVectorizer\n",
    "\n",
    "X_tfidf, vectorizer_tfidf = vectorize_corpus('tfidf', sentiment_corpus, preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tfidf_matrix(X_tfidf, sentiment_corpus, vectorizer_tfidf.get_feature_names_out()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Como se ve, ac√° no es muy posible ver la matriz, ya que es muy grande y aperentemente no tiene sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducci√≥n de dimensionalidad\n",
    "La matriz TF-IDF es grande y dispersa; aunque podr√≠amos intentar reducir el numero de *features* que tomar√° el vectorizador, a los objetivos de la clase resulta m√°s interesante pasar por un algoritmo sencillo de reducci√≥n de dimensionalidad. \n",
    "\n",
    "Para representar la matrix visualmente necesitamos reducirla a dos dimensiones:\n",
    "\n",
    "üîπ **UMAP** (*Uniform Manifold Approximation and Projection*)\n",
    "\n",
    "- Preserva relaciones locales y globales mejor que t-SNE.\n",
    "\n",
    "- Muy √∫til en visualizaciones de alta calidad para embeddings textuales.\n",
    "\n",
    "\n",
    "### Caracteristicas\n",
    "- **Construcci√≥n de un grafo de vecinos**: \n",
    " - se calcula una representaci√≥n de los datos mediante un grafo en el que cada punto de datos est√° conectado a sus vecinos m√°s cercanos. \n",
    " UMAP calcula la probabilidad de que un punto est√© conectado a otro en funci√≥n de la distancia entre ellos en el espacio original de alta dimensi√≥n.\n",
    "- **Optimizaci√≥n de la proyecci√≥n:**\n",
    " Luego, UMAP proyecta los datos de alta dimensi√≥n a un espacio de baja dimensi√≥n, manteniendo la estructura del grafo de vecinos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "umap_reducer = umap.UMAP(random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "X_embedded = umap_reducer.fit_transform(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_embedded[:,0], X_embedded[:,1], s=5, alpha=0.7)\n",
    "plt.title(\"Visualizaci√≥n UMAP de vectores TF-IDF\", fontsize=14)\n",
    "plt.xlabel(\"UMAP Dimensi√≥n 1\")\n",
    "plt.ylabel(\"UMAP Dimensi√≥n 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    \"dim1\": X_embedded[:, 0],\n",
    "    \"dim2\": X_embedded[:, 1],\n",
    "    \"texto\": sentiment_corpus\n",
    "})\n",
    "\n",
    "fig = px.scatter(df_plot, x='dim1', y='dim2', hover_data=['texto'],\n",
    "                 title='UMAP interactivo de vectores TF-IDF')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tfidf_mean = np.asarray(X_tfidf.mean(axis=0)).flatten()\n",
    "terms = vectorizer_tfidf.get_feature_names_out()\n",
    "tfidf_scores = dict(zip(terms, tfidf_mean))\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "wordcloud.generate_from_frequencies(tfidf_scores)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Nube de palabras promedio TF-IDF', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividimos los datos en entrenamiento y prueba\n",
    "# La funci√≥n train_test_split divide los datos en conjuntos de entrenamiento y prueba\n",
    "# Por ahora vamos a utilizarla dos veces, puesto que tenemos dos X uno con tfidf y otro con count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNB + vectorizaci√≥n con count vectorizer\n",
    "\n",
    "X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_count, sentiment_labels, test_size=0.3, random_state=42)\n",
    "model_wiht_cv = MultinomialNB()\n",
    "model_wiht_cv.fit(X_train_cv, y_train_cv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos el modelo\n",
    "y_pred_cv = model_wiht_cv.predict(X_test_cv)\n",
    "print(f\"Accuracy: {accuracy_score(y_test_cv, y_pred_cv):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB + vectorizaci√≥n con tfidf vectorizer\n",
    "# Entrenamos el modelo\n",
    "\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(X_tfidf, sentiment_labels, test_size=0.3, random_state=42)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tf, y_train_tf)\n",
    "\n",
    "# Evaluamos el modelo\n",
    "y_pred_tf = model.predict(X_test_tf)\n",
    "print(f\"Accuracy: {accuracy_score(y_test_tf, y_pred_tf):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nueva_sentence = \"todo muy lindo salvo por la comida que era horrible\"\n",
    "print(model.predict(vectorizer_tfidf.transform([nueva_sentence])))\n",
    "\n",
    "if model.predict(vectorizer_tfidf.transform([nueva_sentence])) == 1:\n",
    "    print(\"Sentimiento positivo\")\n",
    "else: \n",
    "    print(\"Sentimiento negativo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nueva_sentence = \"todo muy lindo salvo por la comida que era horrible\"\n",
    "print(model_wiht_cv.predict(count_vectorizer.transform([nueva_sentence])))\n",
    "\n",
    "if model_wiht_cv.predict(count_vectorizer.transform([nueva_sentence])) == 1:\n",
    "    print(\"Sentimiento positivo\")\n",
    "else: \n",
    "    print(\"Sentimiento negativo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, vectorizer, sentence):\n",
    "    \"\"\"\n",
    "    Predice el sentimiento de una oraci√≥n dada utilizando el modelo y el vectorizador proporcionados.\n",
    "    \"\"\"\n",
    "    preprocessed_sentence = preprocess_text(sentence)\n",
    "    prediction = model.predict(vectorizer.transform([preprocessed_sentence]))\n",
    "    return prediction[0]  # 1 = positivo, 0 = negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_sentiment(model, vectorizer_tfidf, \"La pelicula es malisima, a pesar de los buenos dialogos.\"))\n",
    "\n",
    "print(predict_sentiment(model_wiht_cv, count_vectorizer, \"La pelicula es malisima, a pesar de los buenos dialogos.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df = pd.read_csv(\"synth_eval_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testset_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_df['norm_text'] = testset_df['text'].apply(preprocess_text)\n",
    "testset_df['predicted_label_tfidf'] = testset_df['text'].apply(lambda x: predict_sentiment(model, vectorizer_tfidf, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df['predicted_label_cv'] = testset_df['text'].apply(lambda x: predict_sentiment(model_wiht_cv, count_vectorizer, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report   \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f'Matriz de Confusi√≥n - {model_name}')\n",
    "    plt.xlabel('Predicci√≥n')\n",
    "    plt.ylabel('Real')\n",
    "    plt.xticks(ticks=[0.5, 1.5], labels=['Negativo', 'Positivo'])\n",
    "    plt.yticks(ticks=[0.5, 1.5], labels=['Negativo', 'Positivo'], rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "def make_classification_report(y_true, y_pred, model_name):\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(f\"Reporte de clasificaci√≥n - {model_name}\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(testset_df['label'], testset_df['predicted_label_tfidf'], \"TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_classification_report(testset_df['label'], testset_df['predicted_label_tfidf'], \"TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora veamos los resultados del Count Vectorizer\n",
    "\n",
    "plot_confusion_matrix(testset_df['label'], testset_df['predicted_label_cv'], \"Count Vectorizer\")\n",
    "make_classification_report(testset_df['label'], testset_df['predicted_label_cv'], \"Count Vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
