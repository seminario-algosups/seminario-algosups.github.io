{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoIlc6jqAAa1"
   },
   "source": [
    "# Clase - 07 - Procesamiento morfológico - (Virtual - Sábado 3 de Mayo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sugerencias de uso de la Notebook: \n",
    "- Sugerimos 'Abrir en Colab' y realizar una copia del cuaderno antes de usarlo.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seminario-algosups/seminario-algosups.github.io/blob/master/Clase-07/clase-07.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDEdiaxMAAa2"
   },
   "source": [
    "\n",
    "*   **Conceptos Clave:**\n",
    "    *   **Raíz (Root):** El núcleo léxico irreductible de una palabra una vez que se han eliminado todos los afijos flexivos y derivativos. *Ejemplo: 'port-' en 'portador', 'exportar', 'transporte'.*\n",
    "    *   **Lema (Lemma):** La forma *canónica* o de diccionario de una palabra. Representa el conjunto de todas sus formas flexionadas. *Ejemplo: 'correr' es el lema de 'corro', 'corres', 'corrió', 'corriendo'.*\n",
    "    *   **Tema (Stem):** La parte de la palabra a la que se añaden los afijos flexivos. Puede incluir la raíz y afijos derivativos. *Ejemplo: En español, para verbos, a menudo coincide con la raíz + vocal temática (e.g., 'cant-a-' en 'cantamos'). En inglés, 'running' -> stem 'runn' (con duplicación), 'cats' -> stem 'cat'. (Nota: La definición precisa varía entre teorías y lenguas)*.\n",
    "    *   **Forma de Palabra | Forma léxica (Word Form):** La palabra tal como aparece en el texto. *Ejemplo: 'corrieron', 'gatos', 'beautifully'.*\n",
    "*   **Tipos de Morfología:**\n",
    "    *   **Morfología Flexiva (Inflectional Morphology):** Modifica una palabra para expresar diferentes categorías gramaticales (tiempo, número, género, caso, etc.) sin cambiar su categoría léxica central. *Ejemplo: 'cantar' -> 'cantó' (tiempo), 'gato' -> 'gatos' (número).* El resultado es una *forma de palabra* diferente del mismo *lema*.\n",
    "    *   **Morfología Derivativa (Derivational Morphology):** Crea nuevas palabras (nuevos lemas) a partir de otras, a menudo cambiando la categoría léxica. *Ejemplo: 'nación' (sustantivo) -> 'nacional' (adjetivo) -> 'nacionalizar' (verbo).*\n",
    "\n",
    "En cualquier tarea de NLP (clasificación, similitud, ebeddings, etc.) resulta importante agrupar o normalizar palabras y estas nociones nos ayudan en ese objeticvo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er4sZOMBAAa2"
   },
   "source": [
    "## Procesamiento Morfológico Básico con NLTK\n",
    "\n",
    "[NLTK](https://www.nltk.org/#natural-language-toolkit)  *is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeLfTnKIwBbZ"
   },
   "outputs": [],
   "source": [
    "# (Pre-requisito) Instalación y Descarga de Recursos NLTK\n",
    "\n",
    "#!pip install nltk  # Descomentar y ejecutar si nltk no está instalado en el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lM7UqD8q_xWd",
    "outputId": "70b6d12e-0309-4b96-b061-f30008e9c094"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "# Descargar recursos necesarios (solo la primera vez que se ejecuta en un entorno)\n",
    "print(\"Descargando recursos de NLTK...\")\n",
    "nltk.download('punkt', quiet=True) # Para tokenización\n",
    "nltk.download('punkt_tab', quiet=True) # Español\n",
    "print(\"Punkt descargado.\")\n",
    "nltk.download('wordnet', quiet=True) # Para lematizador WordNet\n",
    "print(\"WordNet descargado.\")\n",
    "nltk.download('omw-1.4', quiet=True) # Open Multilingual Wordnet\n",
    "print(\"OMW 1.4 descargado.\")\n",
    "print(\"Recursos listos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYsP4b-UAAa3"
   },
   "source": [
    "*   **Tokenización:**\n",
    "    *   **Concepto:** Dividir el texto en unidades significativas (palabras, puntuación), llamadas *tokens*. Es el primer paso en la mayoría de los pipelines de NLP.\n",
    "    *   **Implementación NLTK:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5Z0bNq-AMtU",
    "outputId": "2c3b9501-27ed-4c2e-8615-b30aef43b47f"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texto = \"Las ideas verdes incoloras duermen relocas.\"\n",
    "tokens = word_tokenize(texto, language='spanish')\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ8evVC1AAa4"
   },
   "source": [
    "*   **Stemming:**\n",
    "    *   Proceso para reducir las palabras flexionadas a una forma base común o \"raíz\" (stem).\n",
    "    *   A menudo, crudo y basado en reglas o heurísticas.\n",
    "    *   Intenta aproximarse a la raíz o al tema mediante el recorte de los tokens a partir de sufijos comunes.\n",
    "    *     No siempre resulta en una palabra real o en el lema lingüístico correcto.\n",
    "    *     Es rápido pero menos preciso que la lematización.\n",
    "\n",
    "**Implementación NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g85G8U9qANqU",
    "outputId": "0e7e3470-ec9e-41dd-fe80-ce72af76d611"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer_es = SnowballStemmer('spanish')\n",
    "# tokens = ['corriendo', 'corredor', 'correrán', 'casas', 'casero'] # Chequear que  'corredor' y 'casero' (derivación) pueden o no reducirse dependiendo del stemmer.\n",
    "\n",
    "stems = [stemmer_es.stem(t) for t in tokens]\n",
    "print(stems)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o2uMENLAAa4"
   },
   "source": [
    "* Ventajas: velocidad, simplicidad\n",
    "* Desventajas: sobrerreducción, no siempre produce palabras reales, ignora el contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJfM-QNvAAa4"
   },
   "source": [
    "## **Lemmatizing (Lemmatization):**\n",
    "*   Proceso más sofisticado que el stemming, que busca reducir la palabra a su *lema* (forma de diccionario)\n",
    "*   Tiene en consideración el contexto gramatical (categoría gramatical o Part-of-Speech - POS).\n",
    "*   Intenta encontrar el lema lingüístico correcto dado que maneja de forma acertada la flexión y algunos casos de derivación.\n",
    "\n",
    "    \n",
    "### Implementación NLTK (usando WordNet):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_t9_6qXAO05",
    "outputId": "ab77aadc-66f2-48b6-a26d-90e79669e04e"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# El POStag es crucial para el lematizadoer (v=verbo, n=nombre, a=adjetivo, r=adverbio)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"Lematización 'running' (verbo):\", lemmatizer.lemmatize('running', pos='v')) #\n",
    "print(\"Lematización 'ran' (verbo):\", lemmatizer.lemmatize('ran', pos='v'))\n",
    "print(\"Lematización 'cats' (nombre):\", lemmatizer.lemmatize('cats', pos='n'))\n",
    "print(\"Lematización  'better' (adjetivo):\", lemmatizer.lemmatize('better', pos='a'))\n",
    "\n",
    "# Ejemplo sin POS tag (lo asume como  nombre por defecto):\n",
    "print(\"Lematización  'running' (sin POS):\", lemmatizer.lemmatize('running'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0y9XC8iAAa4"
   },
   "source": [
    "Ventajas: produce palabras reales/lemas, más preciso lingüísticamente.\n",
    "Desventajas: más lento, requiere POS tagging para buena precisión, dependencia de lexicones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqe_yxZWAAa4"
   },
   "source": [
    "## Lematización para español con spaCy\n",
    "\n",
    "[spaCy](https://spacy.io)  librería moderna y eficiente para NLP, orientada a la producción, con modelos pre-entrenados para varios idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d90KBRdd3VCo",
    "outputId": "393d8580-bd9e-4600-d5f9-3a05ef0e7ac0"
   },
   "outputs": [],
   "source": [
    "# (Pre-requisito) Instalación de spaCy y Descarga de Modelos\n",
    "\n",
    "# !pip install -q spacy # Instañlá spacy si no está presente (el -q es para modo silencioso)\n",
    "# !python -m spacy download es_core_news_lg # Descarga modelo grande para español (usa _md o _sm para medianos/pequeños)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7n9-R-BAPj9"
   },
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Cargar el modelo de español\n",
    "# Considerá usar modelos más pequeños ('es_core_news_sm', 'es_core_news_md') si los recursos son limitados\n",
    "# Los modelos más grandes ('lg') suelen tener mejor rendimiento en lematización y otras tareas.\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "\n",
    "# Si necesitaras inglés:\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# nlp_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DozkQ7-NAAa5"
   },
   "source": [
    "*   **Procesamiento con spaCy:**\n",
    "    *   spaCy realiza múltiples tareas (tokenización, POS tagging, análisis de dependencias, NER, *lematización*) en un solo paso al procesar el texto con el objeto `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kI1t78_AQVq",
    "outputId": "37582040-b935-4cb9-cb9f-9f22d45723dd"
   },
   "outputs": [],
   "source": [
    "# texto = \"Los corredores corrían rápidamente hacia las casas.\"\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Iterar sobre los tokens y acceder a sus atributos\n",
    "print(f\"{'Token:':<15} {'Lema:':<15} {'POS:'}\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scYAwlvL-IlB"
   },
   "source": [
    "### Comparacion Stem & Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8RKZx5N-Hot",
    "outputId": "c10c372f-b313-4774-a95b-43d0d51d2072"
   },
   "outputs": [],
   "source": [
    "frase_ejercicio = texto # \"Las investigadoras desarrollaron nuevas metodologías analíticas.\"\n",
    "\n",
    "# 2. Tokenizar con NLTK\n",
    "tokens_nltk = word_tokenize(frase_ejercicio, language='spanish')\n",
    "\n",
    "# 3. Stemming con NLTK\n",
    "stemmer_ej = SnowballStemmer('spanish')\n",
    "stems_nltk = [stemmer_ej.stem(token) for token in tokens_nltk]\n",
    "\n",
    "# 4. Procesar con spaCy para lematización\n",
    "doc_spacy = nlp(frase_ejercicio)\n",
    "\n",
    "# Asegurarse de que la tokenización de spaCy coincida o manejar desajustes\n",
    "# spaCy puede tokenizar de forma ligeramente diferente (ej. puntuación)\n",
    "tokens_spacy = [token.text for token in doc_spacy]\n",
    "lemas_spacy = [token.lemma_ for token in doc_spacy]\n",
    "\n",
    "# 5. Imprimir comparación (Alineando resultados si la tokenización difiere)\n",
    "# Una forma simple es iterar sobre los tokens de spaCy ya que incluye puntuación\n",
    "print(f\"{'Token (spaCy)':<15} {'Stem (NLTK)':<15} {'Lema (spaCy)':<15}\")\n",
    "print(\"-\"*45)\n",
    "\n",
    "# Creamos un diccionario de stems NLTK para búsqueda rápida\n",
    "# OJO: Esto asume que word_tokenize y la tokenización de spaCy producen tokens similares\n",
    "# Puede requerir una alineación más compleja en casos difíciles.\n",
    "stem_dict = {token: stem for token, stem in zip(tokens_nltk, stems_nltk)}\n",
    "\n",
    "for token in doc_spacy:\n",
    "    # Intentar encontrar el stem correspondiente del token NLTK\n",
    "    # Se usa token.text.lower() para buscar por si acaso hay diferencias de mayúsculas/minúsculas\n",
    "    # y se provee un default si el token de spaCy (ej. '.') no está en los de NLTK\n",
    "    stem = stem_dict.get(token.text, 'N/A')\n",
    "    if stem == 'N/A' and token.text.lower() in stem_dict:\n",
    "       stem = stem_dict[token.text.lower()]\n",
    "\n",
    "    print(f\"{token.text:<15} {stem:<15} {token.lemma_:<15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnyeGZUZ-QWM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtCGCaP6-SYO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IsI2zCwAAa5"
   },
   "source": [
    "## **Lematización en spaCy: ¿Reglas o Aprendizaje Automático?**\n",
    "spaCy combina diferentes estrategias para la lematización dependiendo del idioma y del modelo.\n",
    "\n",
    "*   **Lematizador Basado en Reglas (Rule-based Lemmatizer):**\n",
    "    *   Utiliza tablas de búsqueda (léxicos) y reglas morfológicas definidas explícitamente para mapear formas flexionadas a lemas.\n",
    "    *   *Ejemplo:* Una regla podría decir \"si una palabra termina en '-ando' o '-iendo' y su POS es VERB, quitar la terminación y añadir 'r' (ajustando la vocal temática si es necesario)\". Una tabla podría listar excepciones como 'soy' -> 'ser'.\n",
    "    *   spaCy permite añadir o modificar estas reglas. Generalmente se usa para excepciones o casos específicos del idioma.\n",
    "*   **Lematizador Basado en Aprendizaje Automático (Machine Learning-based Lemmatizer):**\n",
    "    *   Muchos modelos de spaCy (especialmente los más grandes y recientes como `es_core_news_lg`) utilizan componentes entrenados con aprendizaje automático.\n",
    "    *   Estos modelos aprenden patrones a partir de grandes cantidades de datos anotados (como corpus con lemas correctos) y pueden predecir el lema basándose en la forma de la palabra, su contexto y su POS tag (que también suele predecir un modelo).\n",
    "    *   Suelen ser más robustos ante palabras desconocidas o variaciones no cubiertas por las reglas, pero pueden cometer errores inesperados.\n",
    "    \n",
    "*   **¿Cómo sabe spaCy cuál usar?** La configuración del pipeline del modelo cargado (`nlp.pipe_names`, `nlp.analyze_pipes()`) indica qué componentes están activos. Algunos modelos pueden usar principalmente reglas (a menudo almacenadas en `spacy-lookups-data`), otros pueden usar un componente entrenado (`tagger`, `lemmatizer` que dependen de `tok2vec`), y a menudo es una combinación. Los modelos más grandes (`_md`, `_lg`) tienden a apoyarse más en componentes entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9s7w10WARt_",
    "outputId": "6f419e73-0c3e-476b-e221-4c8a0d2410ec"
   },
   "outputs": [],
   "source": [
    "# Ver los componentes del pipeline actual\n",
    "print(f\"Componentes en el pipeline '{nlp.meta['name']}': {nlp.pipe_names}\")\n",
    "\n",
    "# Análisis más detallado (muestra qué componente asigna qué atributo, como token.lemma)\n",
    "print(\"\\nAnálisis detallado del pipeline:\")\n",
    "print(nlp.analyze_pipes(pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaByuRQdAAa5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-UhuM5NAAa5"
   },
   "source": [
    "## Etiquetamiento en Formato CoNLL-U\n",
    "\n",
    "*   **CoNLL-U:**\n",
    "    *   **¿Qué es?** Un formato de texto plano, basado en columnas separadas por tabuladores, utilizado para anotar corpus lingüísticos. Es un estándar de facto en el proyecto Universal Dependencies y ampliamente usado en NLP.\n",
    "\n",
    "    *   **Estructura:** Una línea por token (o palabra multi-token), columnas fijas con información específica. Comentarios empiezan con `#`. Frases separadas por líneas en blanco.\n",
    "\n",
    "*   **Columnas Relevantes:**\n",
    "    *   **Columna 1: ID:** Índice del token en la frase. Suele ser numérico (1, 2, 3...). Puede indicar rangos para tokens multi-palabra (e.g., 'del' -> 'de el') o números decimales para palabras \"vacías\" insertadas en análisis elípticos.\n",
    "    *   **Columna 2: FORM:** La forma de la palabra tal como aparece en el texto original. Corresponde a la *forma de palabra* discutida en morfología.\n",
    "    *   **Columna 3: LEMMA:** El lema o forma base/diccionario de la palabra. Corresponde al *lema*  o  al resultado esperado de la lematización.\n",
    "    *   **Otras columnas importantes** UPOSTAG (POS universal), XPOSTAG (POS específico del idioma), FEATS (rasgos morfológicos), HEAD (ID del token del que depende), DEPREL (relación de dependencia), DEPS (dependencias secundarias), MISC (anotaciones misceláneas).\n",
    "*   **Ejemplo:**\n",
    "    ```conllu\n",
    "    # sent_id = 1\n",
    "    # text = Los corredores corrían.\n",
    "    1   Los         el          DET    Definite=Def|Gender=Masc|Number=Plur|PronType=Art   2   det     _   _\n",
    "    2   corredores  corredor    NOUN   Gender=Masc|Number=Plur                            3   nsubj   _   _\n",
    "    3   corrían     correr      VERB   Mood=Ind|Number=Plur|Person=3|Tense=Imp|VerbForm=Fin 0   root    _   _\n",
    "    4   .           .           PUNCT  _                                                  3   punct   _   _\n",
    "    ```\n",
    "*   **Análisis del Ejemplo:**\n",
    "    *   En la línea 1: ID=1, FORM=\"Los\", LEMMA=\"el\".\n",
    "    *   En la línea 2: ID=2, FORM=\"corredores\", LEMMA=\"corredor\".\n",
    "    *   En la línea 3: ID=3, FORM=\"corrían\", LEMMA=\"correr\".\n",
    "    *   En la línea 4: ID=4, FORM=\".\", LEMMA=\".\".\n",
    "*   **Relevancia:** Este formato permite almacenar y compartir de manera estandarizada los resultados de análisis como la tokenización (FORM) y la lematización (LEMMA), junto con otra información lingüística valiosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5df-Gb4MAAa5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6pac4lXAAa5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wz3d0_4rAULW",
    "outputId": "9ce1dd1b-46ec-41d0-e633-956908bfaa46"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Instalar la librería si no está\n",
    "# !pip install spacy-conll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dg31np-o5ZYL",
    "outputId": "fdfc2141-4095-4b86-b76e-7b3e7b549118"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "from spacy_conll.formatter import ConllFormatter\n",
    "\n",
    "\n",
    "# --- Añadir el formateador CoNLL-U al pipeline ----\n",
    "# Es importante añadirlo DESPUÉS de los componentes que generan la info necesaria\n",
    "# (parser para dependencias, morphologizer/tagger para POS/feats, lemmatizer)\n",
    "# Si el modelo ya tiene estos componentes, podemos añadirlo al final.\n",
    "\n",
    "if \"conll_formatter\" not in nlp.pipe_names:\n",
    "     # Usar las columnas estándar de CoNLL-U\n",
    "    config = {\"conversion_maps\": {\"dep\": {\"nsubj\": \"nsubj\"}}} # Ejemplo mínimo, a chequear si no es necesario\n",
    "    formatter = nlp.add_pipe(\"conll_formatter\", config=config, last=True)\n",
    "\n",
    "texto_es = texto # \"El rápido zorro marrón salta sobre el perro perezoso.\"\n",
    "doc = nlp(texto_es)\n",
    "\n",
    "# Acceder al formato CoNLL-U a través de la extensión ._.conllu_str\n",
    "conllu_output = doc._.conll_str\n",
    "print(conllu_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBfVxwaTAY1L",
    "outputId": "90960cbe-cd59-4a32-8c77-8d2885127a8e"
   },
   "outputs": [],
   "source": [
    "conllu_output_df = doc._.conll_pd # Lo convertimos en un DF de pandas para facilitar la manipulación y visualización.\n",
    "print(conllu_output_df[['ID', 'FORM', 'LEMMA', 'UPOS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrvTkvk0AjwK",
    "outputId": "0af9b098-0ce5-45bb-d003-bd5211ccd54e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b8_SYWeNmtN"
   },
   "source": [
    "## Parte 2: TP - 1\n",
    "* Objetivo: Analizar cómo diferentes medios presentan la misma información usando herramientas de NLP y el formato CoNLL-U.\n",
    "\n",
    "\n",
    "### Paso 1: Limpieza del Texto:\n",
    "Los textos de noticias online contienen HTML, anuncios, menús, etc. Necesitamos extraer el cuerpo principal de la noticia.\n",
    "\n",
    "* Método Simple:\n",
    " *  Copiar y pegar el texto relevante de cada noticia en archivos de texto plano (.txt) separados (noticia1.txt, noticia2.txt, noticia3.txt).\n",
    " *  Eliminar manualmente encabezados, pies de página, publicidad  u otro tipo de ruido.\n",
    " *  Guardar los archivos respetando los nombres y cargarlos en la notebook. Podés usar el menu de la izquierda.\n",
    "\n",
    "### Generación de Anotaciones CoNLL-U con spaCy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O03riYY-o9BH"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "def preprocess_spanish_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Lee un archivo de texto en español, elimina líneas en blanco y aplica\n",
    "    preprocesamiento básico de NLP.\n",
    "\n",
    "    Pasos de preprocesamiento:\n",
    "    1.  Lee el archivo línea por línea.\n",
    "    2.  Elimina líneas que estén completamente en blanco o contengan solo espacios.\n",
    "    3.  Une las líneas restantes en un solo bloque de texto.\n",
    "    4.  Convierte todo el texto a minúsculas.\n",
    "    5.  Elimina signos de puntuación (reemplazándolos con espacios para evitar unir palabras).\n",
    "    6.  Elimina números (reemplazándolos con espacios).\n",
    "    7.  Elimina espacios en blanco extra (múltiples espacios, tabulaciones, etc.),\n",
    "        dejando solo un espacio entre palabras.\n",
    "    8.  Elimina espacios en blanco al principio y al final del texto resultante.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): La ruta al archivo .txt que se va a procesar.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto preprocesado como una única cadena de texto.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: Si el archivo especificado en file_path no existe.\n",
    "        IOError: Si ocurre un error al leer el archivo.\n",
    "    \"\"\"\n",
    "    input_path = Path(file_path)\n",
    "    if not input_path.is_file():\n",
    "        raise FileNotFoundError(f\"El archivo no fue encontrado en: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        #\n",
    "        with input_path.open('r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"Error al leer el archivo {file_path}: {e}\")\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip() # Elimina espacios/saltos al inicio/final de la línea\n",
    "        if stripped_line: # Si la línea no está vacía después de quitar espacios\n",
    "            cleaned_lines.append(stripped_line)\n",
    "\n",
    "    # Unir con un espacio para asegurar separación entre palabras de líneas adyacentes\n",
    "    full_text = ' '.join(cleaned_lines)\n",
    "    text = full_text.lower()\n",
    "\n",
    "\n",
    "\n",
    "    # Definir puntuación a eliminar!\n",
    "    # Podríamos querer agregar mas  símbolos si es necesario: '–', '—', ... string.punctuations puede ir tmb\n",
    "    punctuation_to_remove = string.punctuation + '¡¿«»“”‘’!?'\n",
    "    # Crear una tabla de traducción: cada carácter de puntuación se mapea a un espacio\n",
    "    translator = str.maketrans(punctuation_to_remove, ' ' * len(punctuation_to_remove))\n",
    "    text = text.translate(translator)\n",
    "    # Reemplazar secuencias de dígitos con un espacio\n",
    "    text = re.sub(r'\\d+', '00', text)\n",
    "    # print(text)\n",
    "    # Reemplazar una o más ocurrencias de espacios/tabs/newlines con un solo espacio\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B46SpUjWNseG",
    "outputId": "3211c9cc-b1d8-4a99-97be-efb6c45fb8e1"
   },
   "outputs": [],
   "source": [
    "# Asegurarse de tener spacy, es_core_news_lg y spacy-conllu cargados en el entorno. Si no, ejecutar más arriba.)\n",
    "# Asegurarse tmb de que los archivos se llamen noticia1.txt y estésn cargados en el entorno.\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "nombres_archivos_txt = [\"noticia1.txt\", \"noticia2.txt\"]\n",
    "nombres_archivos_conllu = [\"noticia1.conllu\", \"noticia2.conllu\"]\n",
    "\n",
    "print(\"Procesando archivos y generando CoNLL-U...\")\n",
    "\n",
    "for txt_file, conllu_file in zip(nombres_archivos_txt, nombres_archivos_conllu):\n",
    "    print(f\"  Procesando {txt_file}...\")\n",
    "    try:\n",
    "        ruta_txt = Path(txt_file)\n",
    "        # texto_noticia = ruta_txt.read_text(encoding=\"utf-8\")\n",
    "        text_norm = preprocess_spanish_text(ruta_txt) # Lo mandamos a preprocesar\n",
    "        doc = nlp(text_norm)\n",
    "\n",
    "        conllu_output = doc._.conll_str # me quedo con el resultado de conllu\n",
    "        ruta_conllu = Path(conllu_file)\n",
    "        ruta_conllu.write_text(conllu_output, encoding=\"utf-8\") # lo guardo\n",
    "        print(f\"    -> Guardado en {conllu_file}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"    ERROR: Archivo {txt_file} no encontrado. Asegúrate de que exista.\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR procesando {txt_file}: {e}\")\n",
    "\n",
    "print(\"Proceso completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJVxuv3ETzn3"
   },
   "outputs": [],
   "source": [
    "#!pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjiYySldSpA8"
   },
   "outputs": [],
   "source": [
    "from conllu import parse_incr\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def analizar_conllu(filepath):\n",
    "    \"\"\"Lee un archivo CoNLL-U y extrae algunas estadísticas.\"\"\"\n",
    "    tokens_data = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for tokenlist in parse_incr(f):\n",
    "            tokens_data.extend(tokenlist) # Acumula tokens de todas las frases\n",
    "\n",
    "    num_tokens = len(tokens_data)\n",
    "    num_sentences = len(set(t['id'][0] for t in tokens_data if isinstance(t['id'], tuple))) # Aproximación contando sent_id si existe, o reinicios de ID=1\n",
    "    if not num_sentences: # Fallback si no hay metadatos o IDs son continuos\n",
    "         num_sentences = sum(1 for t in tokens_data if t['id'] == 1)\n",
    "\n",
    "\n",
    "    lemas = [t['lemma'].lower() for t in tokens_data if t['lemma'] not in stop_words_es]\n",
    "    pos_tags = [t['upos'] for t in tokens_data if t['lemma'] not in stop_words_es]\n",
    "    dep_rels = [t['deprel'] for t in tokens_data if t['lemma'] not in stop_words_es]\n",
    "\n",
    "    # Contar los 10 lemas más comunes (excluyendo puntuación)\n",
    "    lemas_comunes = Counter(l for l, p in zip(lemas, pos_tags) if p != 'PUNCT').most_common(10)\n",
    "    # Contar etiquetas POS\n",
    "    pos_counts = Counter(pos_tags)\n",
    "    # Contar relaciones de dependencia\n",
    "    dep_counts = Counter(dep_rels)\n",
    "\n",
    "    print(f\"\\n--- Análisis de: {filepath} ---\")\n",
    "    print(f\"  Frases (aprox): {num_sentences}\")\n",
    "    print(f\"  Tokens: {num_tokens}\")\n",
    "    if num_sentences > 0:\n",
    "         print(f\"  Tokens/Frase (promedio): {num_tokens/num_sentences:.2f}\")\n",
    "    print(f\"  Lemas más comunes: {lemas_comunes}\")\n",
    "    print(f\"  Distribución POS: {pos_counts}\")\n",
    "    # print(f\"  Distribución DepRel: {dep_counts}\") # Puede ser muy largo\n",
    "\n",
    "    # Podríamos devolver un diccionario o DataFrame para comparar fácilmente\n",
    "    return {\n",
    "        \"Archivo\": filepath,\n",
    "        \"Frases\": num_sentences,\n",
    "        \"Tokens\": num_tokens,\n",
    "        \"Lemas_Top10\": lemas_comunes,\n",
    "        \"POS_Counts\": pos_counts\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDUPYNuMS9V_",
    "outputId": "437de9eb-604e-42a2-cdd2-2863065bb315"
   },
   "outputs": [],
   "source": [
    "# --- Analizar los archivos generados ---\n",
    "resultados = []\n",
    "for conllu_file in nombres_archivos_conllu:\n",
    "     try:\n",
    "         resultados.append(analizar_conllu(conllu_file))\n",
    "     except FileNotFoundError:\n",
    "         print(f\"Archivo {conllu_file} no encontrado para análisis.\")\n",
    "     except Exception as e:\n",
    "         print(f\"Error analizando {conllu_file}: {e}\")\n",
    "\n",
    "#  Mostrar comparación en una tabla con Pandas\n",
    "if resultados:\n",
    "    df = pd.DataFrame(resultados)\n",
    "\n",
    "    # print(df)\n",
    "    for res in resultados:\n",
    "         print(f\"\\nResumen para {res['Archivo']}:\")\n",
    "         print(f\"  Frases: {res['Frases']}, Tokens: {res['Tokens']}\")\n",
    "         print(f\"  Lemas Top 10: {res['Lemas_Top10']}\")\n",
    "         print(f\"  POS Counts (Top 5): {Counter(res['POS_Counts']).most_common(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eK4o0b79p5Dj",
    "outputId": "3ce22ef4-66fa-408a-bbde-bb192d16ca56"
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_lemmas_from_conllu(filepath: str) -> list[str]:\n",
    "    \"\"\"Lee un archivo CoNLL-U y devuelve una lista de lemas (lowercase, no puntuación).\"\"\"\n",
    "    lemmas = []\n",
    "    ruta_archivo = Path(filepath)\n",
    "    if not ruta_archivo.is_file():\n",
    "        print(f\"Advertencia: Archivo no encontrado {filepath}\")\n",
    "        return []\n",
    "    try:\n",
    "        with ruta_archivo.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for tokenlist in parse_incr(f):\n",
    "                for token in tokenlist:\n",
    "                    # Usar .get si falta la columna\n",
    "                    lemma = token.get('lemma')\n",
    "                    upos = token.get('upos')\n",
    "                    # Añadir si el lema existe y no es puntuación, si no se saltea ese lema\n",
    "                    if lemma and upos != 'PUNCT':\n",
    "                        lemmas.append(lemma.lower())\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {filepath}: {e}\")\n",
    "        return []\n",
    "    return lemmas\n",
    "\n",
    "# Asumiendo que los archivos .conllu se llaman como antes!!\n",
    "nombres_archivos_conllu = [\"noticia1.conllu\", \"noticia2.conllu\"]\n",
    "lista_de_lemmas_por_noticia = []\n",
    "corpus_para_tfidf = []\n",
    "\n",
    "print(\"Extrayendo lemas de archivos CONLLU...\")\n",
    "for conllu_file in nombres_archivos_conllu:\n",
    "    lemmas_noticia = extract_lemmas_from_conllu(conllu_file)\n",
    "    if lemmas_noticia:\n",
    "        lista_de_lemmas_por_noticia.append(lemmas_noticia)\n",
    "        # Unir los lemas en un string para tfidfVectorizer\n",
    "        corpus_para_tfidf.append(\" \".join(lemmas_noticia))\n",
    "        print(f\"  - Extraídos {len(lemmas_noticia)} lemas de {conllu_file}\")\n",
    "    else:\n",
    "        print(f\"  - No se pudieron extraer lemas de {conllu_file}\")\n",
    "       # handlear la ausencia de lenmas\n",
    "        corpus_para_tfidf.append(\"\") # Añadir string vacío si falla\n",
    "\n",
    "# Verificar que tenemos datos para procesar\n",
    "if not any(corpus_para_tfidf):\n",
    "    print(\"\\nError: No se pudieron obtener lemas de ningún archivo. No se puede continuar con TF-IDF.\")\n",
    "elif len([c for c in corpus_para_tfidf if c]) < 2:\n",
    "     print(\"\\nAdvertencia: Se necesita al menos 2 documentos con contenido para calcular IDF. Los resultados pueden no ser significativos.\")\n",
    "     # Podrías optar por no ejecutar TF-IDF en este caso\n",
    "else:\n",
    "    print(\"\\nLemas listos para análisis TF-IDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxmmhCHpsxE8"
   },
   "outputs": [],
   "source": [
    "#nltk.download('stopwords', quiet=True) # Bajamos las stopwords de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y145EzNn5RVO",
    "outputId": "fe7897b9-fcd8-48e1-e576-a49a6d894fe0"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "tfidf_matrix = None\n",
    "feature_names = []\n",
    "vectorizer = None\n",
    "\n",
    "# Solo proceder si tenemos al menos un documento con contenido\n",
    "# (Idealmente, necesitamos al menos 2 para que el idf tenga sentido)\n",
    "if any(corpus_para_tfidf) and len([c for c in corpus_para_tfidf if c]) >= 1:\n",
    "    print(\"Calculando TF-IDF...\")\n",
    "\n",
    "    stop_words_es = stop_words_es = stopwords.words('spanish')\n",
    "    print(f\"  Se usarán {len(stop_words_es)} stop words de NLTK.\")\n",
    "        # También podemos agregar sw:\n",
    "        # stop_words_es.extend(['palabra_extra1', 'palabra_extra2'])\n",
    "    vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, norm='l2', stop_words=stop_words_es)\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus_para_tfidf)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    print(f\"Matriz TF-IDF calculada: {tfidf_matrix.shape[0]} documentos, {tfidf_matrix.shape[1]} lemas únicos.\")\n",
    "else:\n",
    "    print(\"No hay suficientes datos válidos para calcular TF-IDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6h1IwmsSkIB",
    "outputId": "44d7d0b3-e0be-4439-b51c-5a0fe677ca43"
   },
   "outputs": [],
   "source": [
    "def get_top_tfidf_lemmas(doc_index, tfidf_matrix, feature_names, top_n=10):\n",
    "    \"\"\"Obtiene los top N lemas para un documento específico.\"\"\"\n",
    "    # Asegurarse de que la matriz y los nombres existan\n",
    "    if tfidf_matrix is None or not feature_names.any():\n",
    "         return []\n",
    "\n",
    "    doc_scores = tfidf_matrix[doc_index].toarray().flatten()\n",
    "    # Obtenemos índices ordenados de mayor a menor score\n",
    "    sorted_indices = np.argsort(doc_scores)[::-1]\n",
    "    # Seleccionar los top N índices (asegurarse de no exceder el número de features)\n",
    "    top_indices = sorted_indices[:min(top_n, len(feature_names))]\n",
    "    # Obtengo tmb  los lemas y scores correspondientes\n",
    "    top_lemmas_scores = [(feature_names[i], doc_scores[i]) for i in top_indices if doc_scores[i] > 0] # Solo incluir si score > 0\n",
    "    return top_lemmas_scores\n",
    "\n",
    "# Mostrar los resultados si se calculó TF-IDF.\n",
    "if tfidf_matrix is not None:\n",
    "    print(\"\\n--- Lemas más Característicos por Noticia (según TF-IDF) ---\")\n",
    "    for i, filepath in enumerate(nombres_archivos_conllu):\n",
    "        # Solo mostrar si el documento original tenía contenido\n",
    "        if corpus_para_tfidf[i]:\n",
    "            print(f\"\\nNoticia: {filepath}\")\n",
    "            top_lemmas = get_top_tfidf_lemmas(i, tfidf_matrix, feature_names, top_n=15) # Pedir 15, por ejemplo\n",
    "            if top_lemmas:\n",
    "                for lemma, score in top_lemmas:\n",
    "                    print(f\"  - {lemma:<20} (Score: {score:.4f})\")\n",
    "            else:\n",
    "                print(\"  No se encontraron lemas con score TF-IDF > 0.\")\n",
    "        else:\n",
    "             print(f\"\\nNoticia: {filepath} (Archivo vacío o no procesado, se omite)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnSpZ-ezrz6u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{% include additional_content.html %}\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
