{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoIlc6jqAAa1"
      },
      "source": [
        "# Clase - 07 - Procesamiento morfológico - (Virtual - Sábado 3 de Mayo)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDEdiaxMAAa2"
      },
      "source": [
        "\n",
        "*   **Conceptos Clave:**\n",
        "    *   **Raíz (Root):** El núcleo léxico irreductible de una palabra una vez que se han eliminado todos los afijos flexivos y derivativos. *Ejemplo: 'port-' en 'portador', 'exportar', 'transporte'.*\n",
        "    *   **Lema (Lemma):** La forma *canónica* o de diccionario de una palabra. Representa el conjunto de todas sus formas flexionadas. *Ejemplo: 'correr' es el lema de 'corro', 'corres', 'corrió', 'corriendo'.*\n",
        "    *   **Tema (Stem):** La parte de la palabra a la que se añaden los afijos flexivos. Puede incluir la raíz y afijos derivativos. *Ejemplo: En español, para verbos, a menudo coincide con la raíz + vocal temática (e.g., 'cant-a-' en 'cantamos'). En inglés, 'running' -> stem 'runn' (con duplicación), 'cats' -> stem 'cat'. (Nota: La definición precisa varía entre teorías y lenguas)*.\n",
        "    *   **Forma de Palabra | Forma léxica (Word Form):** La palabra tal como aparece en el texto. *Ejemplo: 'corrieron', 'gatos', 'beautifully'.*\n",
        "*   **Tipos de Morfología:**\n",
        "    *   **Morfología Flexiva (Inflectional Morphology):** Modifica una palabra para expresar diferentes categorías gramaticales (tiempo, número, género, caso, etc.) sin cambiar su categoría léxica central. *Ejemplo: 'cantar' -> 'cantó' (tiempo), 'gato' -> 'gatos' (número).* El resultado es una *forma de palabra* diferente del mismo *lema*.\n",
        "    *   **Morfología Derivativa (Derivational Morphology):** Crea nuevas palabras (nuevos lemas) a partir de otras, a menudo cambiando la categoría léxica. *Ejemplo: 'nación' (sustantivo) -> 'nacional' (adjetivo) -> 'nacionalizar' (verbo).*\n",
        "\n",
        "En cualquier tarea de NLP (clasificación, similitud, ebeddings, etc.) resulta importante agrupar o normalizar palabras y estas nociones nos ayudan en ese objeticvo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er4sZOMBAAa2"
      },
      "source": [
        "## Procesamiento Morfológico Básico con NLTK\n",
        "\n",
        "[NLTK](https://www.nltk.org/#natural-language-toolkit)  *is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeLfTnKIwBbZ"
      },
      "outputs": [],
      "source": [
        "# (Pre-requisito) Instalación y Descarga de Recursos NLTK\n",
        "\n",
        "#!pip install nltk  # Descomentar y ejecutar si nltk no está instalado en el entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM7UqD8q_xWd",
        "outputId": "70b6d12e-0309-4b96-b061-f30008e9c094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Descargando recursos de NLTK...\n",
            "Punkt descargado.\n",
            "WordNet descargado.\n",
            "OMW 1.4 descargado.\n",
            "Recursos listos.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "\n",
        "# Descargar recursos necesarios (solo la primera vez que se ejecuta en un entorno)\n",
        "print(\"Descargando recursos de NLTK...\")\n",
        "nltk.download('punkt', quiet=True) # Para tokenización\n",
        "nltk.download('punkt_tab', quiet=True) # Español\n",
        "print(\"Punkt descargado.\")\n",
        "nltk.download('wordnet', quiet=True) # Para lematizador WordNet\n",
        "print(\"WordNet descargado.\")\n",
        "nltk.download('omw-1.4', quiet=True) # Open Multilingual Wordnet\n",
        "print(\"OMW 1.4 descargado.\")\n",
        "print(\"Recursos listos.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYsP4b-UAAa3"
      },
      "source": [
        "*   **Tokenización:**\n",
        "    *   **Concepto:** Dividir el texto en unidades significativas (palabras, puntuación), llamadas *tokens*. Es el primer paso en la mayoría de los pipelines de NLP.\n",
        "    *   **Implementación NLTK:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5Z0bNq-AMtU",
        "outputId": "2c3b9501-27ed-4c2e-8615-b30aef43b47f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Las', 'ideas', 'verdes', 'incoloras', 'duermen', 'relocas', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texto = \"Las ideas verdes incoloras duermen relocas.\"\n",
        "tokens = word_tokenize(texto, language='spanish')\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ8evVC1AAa4"
      },
      "source": [
        "*   **Stemming:**\n",
        "    *   Proceso para reducir las palabras flexionadas a una forma base común o \"raíz\" (stem).\n",
        "    *   A menudo, crudo y basado en reglas o heurísticas.\n",
        "    *   Intenta aproximarse a la raíz o al tema mediante el recorte de los tokens a partir de sufijos comunes.\n",
        "    *     No siempre resulta en una palabra real o en el lema lingüístico correcto.\n",
        "    *     Es rápido pero menos preciso que la lematización.\n",
        "\n",
        "**Implementación NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g85G8U9qANqU",
        "outputId": "0e7e3470-ec9e-41dd-fe80-ce72af76d611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['las', 'ide', 'verd', 'incolor', 'duerm', 'reloc', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer_es = SnowballStemmer('spanish')\n",
        "# tokens = ['corriendo', 'corredor', 'correrán', 'casas', 'casero'] # Chequear que  'corredor' y 'casero' (derivación) pueden o no reducirse dependiendo del stemmer.\n",
        "\n",
        "stems = [stemmer_es.stem(t) for t in tokens]\n",
        "print(stems)\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o2uMENLAAa4"
      },
      "source": [
        "* Ventajas: velocidad, simplicidad\n",
        "* Desventajas: sobrerreducción, no siempre produce palabras reales, ignora el contexto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJfM-QNvAAa4"
      },
      "source": [
        "## **Lemmatizing (Lemmatization):**\n",
        "*   Proceso más sofisticado que el stemming, que busca reducir la palabra a su *lema* (forma de diccionario)\n",
        "*   Tiene en consideración el contexto gramatical (categoría gramatical o Part-of-Speech - POS).\n",
        "*   Intenta encontrar el lema lingüístico correcto dado que maneja de forma acertada la flexión y algunos casos de derivación.\n",
        "\n",
        "    \n",
        "### Implementación NLTK (usando WordNet):\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_t9_6qXAO05",
        "outputId": "ab77aadc-66f2-48b6-a26d-90e79669e04e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lematización 'running' (verbo): run\n",
            "Lematización 'ran' (verbo): run\n",
            "Lematización 'cats' (nombre): cat\n",
            "Lematización  'better' (adjetivo): good\n",
            "Lematización  'running' (sin POS): running\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# El POStag es crucial para el lematizadoer (v=verbo, n=nombre, a=adjetivo, r=adverbio)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"Lematización 'running' (verbo):\", lemmatizer.lemmatize('running', pos='v')) #\n",
        "print(\"Lematización 'ran' (verbo):\", lemmatizer.lemmatize('ran', pos='v'))\n",
        "print(\"Lematización 'cats' (nombre):\", lemmatizer.lemmatize('cats', pos='n'))\n",
        "print(\"Lematización  'better' (adjetivo):\", lemmatizer.lemmatize('better', pos='a'))\n",
        "\n",
        "# Ejemplo sin POS tag (lo asume como  nombre por defecto):\n",
        "print(\"Lematización  'running' (sin POS):\", lemmatizer.lemmatize('running'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0y9XC8iAAa4"
      },
      "source": [
        "Ventajas: produce palabras reales/lemas, más preciso lingüísticamente.\n",
        "Desventajas: más lento, requiere POS tagging para buena precisión, dependencia de lexicones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqe_yxZWAAa4"
      },
      "source": [
        "## Lematización para español con spaCy\n",
        "\n",
        "[spaCy](https://spacy.io)  librería moderna y eficiente para NLP, orientada a la producción, con modelos pre-entrenados para varios idiomas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d90KBRdd3VCo",
        "outputId": "393d8580-bd9e-4600-d5f9-3a05ef0e7ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting es-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.8.0/es_core_news_lg-3.8.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# (Pre-requisito) Instalación de spaCy y Descarga de Modelos\n",
        "\n",
        "# !pip install -q spacy # Instañlá spacy si no está presente (el -q es para modo silencioso)\n",
        "# !python -m spacy download es_core_news_lg # Descarga modelo grande para español (usa _md o _sm para medianos/pequeños)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7n9-R-BAPj9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import spacy\n",
        "\n",
        "# Cargar el modelo de español\n",
        "# Considerá usar modelos más pequeños ('es_core_news_sm', 'es_core_news_md') si los recursos son limitados\n",
        "# Los modelos más grandes ('lg') suelen tener mejor rendimiento en lematización y otras tareas.\n",
        "nlp = spacy.load('es_core_news_lg')\n",
        "\n",
        "# Si necesitaras inglés:\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# nlp_en = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DozkQ7-NAAa5"
      },
      "source": [
        "*   **Procesamiento con spaCy:**\n",
        "    *   spaCy realiza múltiples tareas (tokenización, POS tagging, análisis de dependencias, NER, *lematización*) en un solo paso al procesar el texto con el objeto `nlp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kI1t78_AQVq",
        "outputId": "37582040-b935-4cb9-cb9f-9f22d45723dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token:          Lema:           POS:\n",
            "Las             el              DET\n",
            "ideas           idea            NOUN\n",
            "verdes          verde           ADJ\n",
            "incoloras       incolora        NOUN\n",
            "duermen         dormir          VERB\n",
            "relocas         reloca          ADJ\n",
            ".               .               PUNCT\n"
          ]
        }
      ],
      "source": [
        "# texto = \"Los corredores corrían rápidamente hacia las casas.\"\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Iterar sobre los tokens y acceder a sus atributos\n",
        "print(f\"{'Token:':<15} {'Lema:':<15} {'POS:'}\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scYAwlvL-IlB"
      },
      "source": [
        "### Comparacion Stem & Lemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8RKZx5N-Hot",
        "outputId": "c10c372f-b313-4774-a95b-43d0d51d2072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token (spaCy)   Stem (NLTK)     Lema (spaCy)   \n",
            "---------------------------------------------\n",
            "Las             las             el             \n",
            "ideas           ide             idea           \n",
            "verdes          verd            verde          \n",
            "incoloras       incolor         incolora       \n",
            "duermen         duerm           dormir         \n",
            "relocas         reloc           reloca         \n",
            ".               .               .              \n"
          ]
        }
      ],
      "source": [
        "frase_ejercicio = texto # \"Las investigadoras desarrollaron nuevas metodologías analíticas.\"\n",
        "\n",
        "# 2. Tokenizar con NLTK\n",
        "tokens_nltk = word_tokenize(frase_ejercicio, language='spanish')\n",
        "\n",
        "# 3. Stemming con NLTK\n",
        "stemmer_ej = SnowballStemmer('spanish')\n",
        "stems_nltk = [stemmer_ej.stem(token) for token in tokens_nltk]\n",
        "\n",
        "# 4. Procesar con spaCy para lematización\n",
        "doc_spacy = nlp(frase_ejercicio)\n",
        "\n",
        "# Asegurarse de que la tokenización de spaCy coincida o manejar desajustes\n",
        "# spaCy puede tokenizar de forma ligeramente diferente (ej. puntuación)\n",
        "tokens_spacy = [token.text for token in doc_spacy]\n",
        "lemas_spacy = [token.lemma_ for token in doc_spacy]\n",
        "\n",
        "# 5. Imprimir comparación (Alineando resultados si la tokenización difiere)\n",
        "# Una forma simple es iterar sobre los tokens de spaCy ya que incluye puntuación\n",
        "print(f\"{'Token (spaCy)':<15} {'Stem (NLTK)':<15} {'Lema (spaCy)':<15}\")\n",
        "print(\"-\"*45)\n",
        "\n",
        "# Creamos un diccionario de stems NLTK para búsqueda rápida\n",
        "# OJO: Esto asume que word_tokenize y la tokenización de spaCy producen tokens similares\n",
        "# Puede requerir una alineación más compleja en casos difíciles.\n",
        "stem_dict = {token: stem for token, stem in zip(tokens_nltk, stems_nltk)}\n",
        "\n",
        "for token in doc_spacy:\n",
        "    # Intentar encontrar el stem correspondiente del token NLTK\n",
        "    # Se usa token.text.lower() para buscar por si acaso hay diferencias de mayúsculas/minúsculas\n",
        "    # y se provee un default si el token de spaCy (ej. '.') no está en los de NLTK\n",
        "    stem = stem_dict.get(token.text, 'N/A')\n",
        "    if stem == 'N/A' and token.text.lower() in stem_dict:\n",
        "       stem = stem_dict[token.text.lower()]\n",
        "\n",
        "    print(f\"{token.text:<15} {stem:<15} {token.lemma_:<15}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnyeGZUZ-QWM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtCGCaP6-SYO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IsI2zCwAAa5"
      },
      "source": [
        "## **Lematización en spaCy: ¿Reglas o Aprendizaje Automático?**\n",
        "spaCy combina diferentes estrategias para la lematización dependiendo del idioma y del modelo.\n",
        "\n",
        "*   **Lematizador Basado en Reglas (Rule-based Lemmatizer):**\n",
        "    *   Utiliza tablas de búsqueda (léxicos) y reglas morfológicas definidas explícitamente para mapear formas flexionadas a lemas.\n",
        "    *   *Ejemplo:* Una regla podría decir \"si una palabra termina en '-ando' o '-iendo' y su POS es VERB, quitar la terminación y añadir 'r' (ajustando la vocal temática si es necesario)\". Una tabla podría listar excepciones como 'soy' -> 'ser'.\n",
        "    *   spaCy permite añadir o modificar estas reglas. Generalmente se usa para excepciones o casos específicos del idioma.\n",
        "*   **Lematizador Basado en Aprendizaje Automático (Machine Learning-based Lemmatizer):**\n",
        "    *   Muchos modelos de spaCy (especialmente los más grandes y recientes como `es_core_news_lg`) utilizan componentes entrenados con aprendizaje automático.\n",
        "    *   Estos modelos aprenden patrones a partir de grandes cantidades de datos anotados (como corpus con lemas correctos) y pueden predecir el lema basándose en la forma de la palabra, su contexto y su POS tag (que también suele predecir un modelo).\n",
        "    *   Suelen ser más robustos ante palabras desconocidas o variaciones no cubiertas por las reglas, pero pueden cometer errores inesperados.\n",
        "    \n",
        "*   **¿Cómo sabe spaCy cuál usar?** La configuración del pipeline del modelo cargado (`nlp.pipe_names`, `nlp.analyze_pipes()`) indica qué componentes están activos. Algunos modelos pueden usar principalmente reglas (a menudo almacenadas en `spacy-lookups-data`), otros pueden usar un componente entrenado (`tagger`, `lemmatizer` que dependen de `tok2vec`), y a menudo es una combinación. Los modelos más grandes (`_md`, `_lg`) tienden a apoyarse más en componentes entrenados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9s7w10WARt_",
        "outputId": "6f419e73-0c3e-476b-e221-4c8a0d2410ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Componentes en el pipeline 'core_news_lg': ['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "\n",
            "Análisis detallado del pipeline:\n",
            "\u001b[1m\n",
            "============================= Pipeline Overview =============================\u001b[0m\n",
            "\n",
            "#   Component         Assigns               Requires   Scores             Retokenizes\n",
            "-   ---------------   -------------------   --------   ----------------   -----------\n",
            "0   tok2vec           doc.tensor                                          False      \n",
            "                                                                                     \n",
            "1   morphologizer     token.morph                      pos_acc            False      \n",
            "                      token.pos                        morph_acc                     \n",
            "                                                       morph_per_feat                \n",
            "                                                                                     \n",
            "2   parser            token.dep                        dep_uas            False      \n",
            "                      token.head                       dep_las                       \n",
            "                      token.is_sent_start              dep_las_per_type              \n",
            "                      doc.sents                        sents_p                       \n",
            "                                                       sents_r                       \n",
            "                                                       sents_f                       \n",
            "                                                                                     \n",
            "3   attribute_ruler                                                       False      \n",
            "                                                                                     \n",
            "4   lemmatizer        token.lemma                      lemma_acc          False      \n",
            "                                                                                     \n",
            "5   ner               doc.ents                         ents_f             False      \n",
            "                      token.ent_iob                    ents_p                        \n",
            "                      token.ent_type                   ents_r                        \n",
            "                                                       ents_per_type                 \n",
            "\n",
            "\u001b[38;5;2m✔ No problems found.\u001b[0m\n",
            "{'summary': {'tok2vec': {'assigns': ['doc.tensor'], 'requires': [], 'scores': [], 'retokenizes': False}, 'morphologizer': {'assigns': ['token.morph', 'token.pos'], 'requires': [], 'scores': ['pos_acc', 'morph_acc', 'morph_per_feat'], 'retokenizes': False}, 'parser': {'assigns': ['token.dep', 'token.head', 'token.is_sent_start', 'doc.sents'], 'requires': [], 'scores': ['dep_uas', 'dep_las', 'dep_las_per_type', 'sents_p', 'sents_r', 'sents_f'], 'retokenizes': False}, 'attribute_ruler': {'assigns': [], 'requires': [], 'scores': [], 'retokenizes': False}, 'lemmatizer': {'assigns': ['token.lemma'], 'requires': [], 'scores': ['lemma_acc'], 'retokenizes': False}, 'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'], 'requires': [], 'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'], 'retokenizes': False}}, 'problems': {'tok2vec': [], 'morphologizer': [], 'parser': [], 'attribute_ruler': [], 'lemmatizer': [], 'ner': []}, 'attrs': {'doc.ents': {'assigns': ['ner'], 'requires': []}, 'doc.sents': {'assigns': ['parser'], 'requires': []}, 'token.pos': {'assigns': ['morphologizer'], 'requires': []}, 'token.head': {'assigns': ['parser'], 'requires': []}, 'token.is_sent_start': {'assigns': ['parser'], 'requires': []}, 'token.dep': {'assigns': ['parser'], 'requires': []}, 'doc.tensor': {'assigns': ['tok2vec'], 'requires': []}, 'token.morph': {'assigns': ['morphologizer'], 'requires': []}, 'token.ent_iob': {'assigns': ['ner'], 'requires': []}, 'token.lemma': {'assigns': ['lemmatizer'], 'requires': []}, 'token.ent_type': {'assigns': ['ner'], 'requires': []}}}\n"
          ]
        }
      ],
      "source": [
        "# Ver los componentes del pipeline actual\n",
        "print(f\"Componentes en el pipeline '{nlp.meta['name']}': {nlp.pipe_names}\")\n",
        "\n",
        "# Análisis más detallado (muestra qué componente asigna qué atributo, como token.lemma)\n",
        "print(\"\\nAnálisis detallado del pipeline:\")\n",
        "print(nlp.analyze_pipes(pretty=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaByuRQdAAa5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-UhuM5NAAa5"
      },
      "source": [
        "## Etiquetamiento en Formato CoNLL-U\n",
        "\n",
        "*   **CoNLL-U:**\n",
        "    *   **¿Qué es?** Un formato de texto plano, basado en columnas separadas por tabuladores, utilizado para anotar corpus lingüísticos. Es un estándar de facto en el proyecto Universal Dependencies y ampliamente usado en NLP.\n",
        "\n",
        "    *   **Estructura:** Una línea por token (o palabra multi-token), columnas fijas con información específica. Comentarios empiezan con `#`. Frases separadas por líneas en blanco.\n",
        "\n",
        "*   **Columnas Relevantes:**\n",
        "    *   **Columna 1: ID:** Índice del token en la frase. Suele ser numérico (1, 2, 3...). Puede indicar rangos para tokens multi-palabra (e.g., 'del' -> 'de el') o números decimales para palabras \"vacías\" insertadas en análisis elípticos.\n",
        "    *   **Columna 2: FORM:** La forma de la palabra tal como aparece en el texto original. Corresponde a la *forma de palabra* discutida en morfología.\n",
        "    *   **Columna 3: LEMMA:** El lema o forma base/diccionario de la palabra. Corresponde al *lema*  o  al resultado esperado de la lematización.\n",
        "    *   **Otras columnas importantes** UPOSTAG (POS universal), XPOSTAG (POS específico del idioma), FEATS (rasgos morfológicos), HEAD (ID del token del que depende), DEPREL (relación de dependencia), DEPS (dependencias secundarias), MISC (anotaciones misceláneas).\n",
        "*   **Ejemplo:**\n",
        "    ```conllu\n",
        "    # sent_id = 1\n",
        "    # text = Los corredores corrían.\n",
        "    1   Los         el          DET    Definite=Def|Gender=Masc|Number=Plur|PronType=Art   2   det     _   _\n",
        "    2   corredores  corredor    NOUN   Gender=Masc|Number=Plur                            3   nsubj   _   _\n",
        "    3   corrían     correr      VERB   Mood=Ind|Number=Plur|Person=3|Tense=Imp|VerbForm=Fin 0   root    _   _\n",
        "    4   .           .           PUNCT  _                                                  3   punct   _   _\n",
        "    ```\n",
        "*   **Análisis del Ejemplo:**\n",
        "    *   En la línea 1: ID=1, FORM=\"Los\", LEMMA=\"el\".\n",
        "    *   En la línea 2: ID=2, FORM=\"corredores\", LEMMA=\"corredor\".\n",
        "    *   En la línea 3: ID=3, FORM=\"corrían\", LEMMA=\"correr\".\n",
        "    *   En la línea 4: ID=4, FORM=\".\", LEMMA=\".\".\n",
        "*   **Relevancia:** Este formato permite almacenar y compartir de manera estandarizada los resultados de análisis como la tokenización (FORM) y la lematización (LEMMA), junto con otra información lingüística valiosa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5df-Gb4MAAa5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6pac4lXAAa5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz3d0_4rAULW",
        "outputId": "9ce1dd1b-46ec-41d0-e633-956908bfaa46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy-conll in /usr/local/lib/python3.11/dist-packages (4.0.1)\n",
            "Requirement already satisfied: spacy>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from spacy-conll) (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0.1->spacy-conll) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->spacy-conll) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->spacy-conll) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->spacy-conll) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->spacy-conll) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->spacy-conll) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->spacy-conll) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->spacy-conll) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->spacy-conll) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->spacy-conll) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.1->spacy-conll) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.1->spacy-conll) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->spacy-conll) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->spacy-conll) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->spacy-conll) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.1->spacy-conll) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.1->spacy-conll) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=3.0.1->spacy-conll) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->spacy-conll) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->spacy-conll) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->spacy-conll) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0.1->spacy-conll) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->spacy-conll) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Instalar la librería si no está\n",
        "# !pip install spacy-conll\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dg31np-o5ZYL",
        "outputId": "fdfc2141-4095-4b86-b76e-7b3e7b549118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\tLas\tel\tDET\tDET\tDefinite=Def|Gender=Fem|Number=Plur|PronType=Art\t2\tdet\t_\t_\n",
            "2\tideas\tidea\tNOUN\tNOUN\tGender=Fem|Number=Plur\t5\tnsubj\t_\t_\n",
            "3\tverdes\tverde\tADJ\tADJ\tNumber=Plur\t2\tamod\t_\t_\n",
            "4\tincoloras\tincolora\tNOUN\tNOUN\tGender=Fem|Number=Plur\t2\tamod\t_\t_\n",
            "5\tduermen\tdormir\tVERB\tVERB\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\t0\tROOT\t_\t_\n",
            "6\trelocas\treloca\tADJ\tADJ\tGender=Fem|Number=Plur\t5\tobj\t_\tSpaceAfter=No\n",
            "7\t.\t.\tPUNCT\tPUNCT\tPunctType=Peri\t5\tpunct\t_\tSpaceAfter=No\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import spacy\n",
        "from spacy_conll.formatter import ConllFormatter\n",
        "\n",
        "\n",
        "# --- Añadir el formateador CoNLL-U al pipeline ----\n",
        "# Es importante añadirlo DESPUÉS de los componentes que generan la info necesaria\n",
        "# (parser para dependencias, morphologizer/tagger para POS/feats, lemmatizer)\n",
        "# Si el modelo ya tiene estos componentes, podemos añadirlo al final.\n",
        "\n",
        "if \"conll_formatter\" not in nlp.pipe_names:\n",
        "     # Usar las columnas estándar de CoNLL-U\n",
        "    config = {\"conversion_maps\": {\"dep\": {\"nsubj\": \"nsubj\"}}} # Ejemplo mínimo, a chequear si no es necesario\n",
        "    formatter = nlp.add_pipe(\"conll_formatter\", config=config, last=True)\n",
        "\n",
        "texto_es = texto # \"El rápido zorro marrón salta sobre el perro perezoso.\"\n",
        "doc = nlp(texto_es)\n",
        "\n",
        "# Acceder al formato CoNLL-U a través de la extensión ._.conllu_str\n",
        "conllu_output = doc._.conll_str\n",
        "print(conllu_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBfVxwaTAY1L",
        "outputId": "90960cbe-cd59-4a32-8c77-8d2885127a8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ID       FORM     LEMMA   UPOS\n",
            "0   1        Las        el    DET\n",
            "1   2      ideas      idea   NOUN\n",
            "2   3     verdes     verde    ADJ\n",
            "3   4  incoloras  incolora   NOUN\n",
            "4   5    duermen    dormir   VERB\n",
            "5   6    relocas    reloca    ADJ\n",
            "6   7          .         .  PUNCT\n"
          ]
        }
      ],
      "source": [
        "conllu_output_df = doc._.conll_pd # Lo convertimos en un DF de pandas para facilitar la manipulación y visualización.\n",
        "print(conllu_output_df[['ID', 'FORM', 'LEMMA', 'UPOS']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrvTkvk0AjwK",
        "outputId": "0af9b098-0ce5-45bb-d003-bd5211ccd54e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ID      FORM     LEMMA   UPOS\n",
            "0   1        El        el    DET\n",
            "1   2    rápido    rápido    ADJ\n",
            "2   3     zorro     zorro   NOUN\n",
            "3   4    marrón    marrón    ADJ\n",
            "4   5     salta     salta  PROPN\n",
            "5   6     sobre     sobre    ADP\n",
            "6   7        el        el    DET\n",
            "7   8     perro     perro  PROPN\n",
            "8   9  perezoso  perezoso    ADJ\n",
            "9  10         .         .  PUNCT\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b8_SYWeNmtN"
      },
      "source": [
        "## Parte 2: TP - 1\n",
        "* Objetivo: Analizar cómo diferentes medios presentan la misma información usando herramientas de NLP y el formato CoNLL-U.\n",
        "\n",
        "\n",
        "### Paso 1: Limpieza del Texto:\n",
        "Los textos de noticias online contienen HTML, anuncios, menús, etc. Necesitamos extraer el cuerpo principal de la noticia.\n",
        "\n",
        "* Método Simple:\n",
        " *  Copiar y pegar el texto relevante de cada noticia en archivos de texto plano (.txt) separados (noticia1.txt, noticia2.txt, noticia3.txt).\n",
        " *  Eliminar manualmente encabezados, pies de página, publicidad  u otro tipo de ruido.\n",
        " *  Guardar los archivos respetando los nombres y cargarlos en la notebook. Podés usar el menu de la izquierda.\n",
        "\n",
        "### Generación de Anotaciones CoNLL-U con spaCy:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O03riYY-o9BH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from pathlib import Path\n",
        "\n",
        "def preprocess_spanish_text(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Lee un archivo de texto en español, elimina líneas en blanco y aplica\n",
        "    preprocesamiento básico de NLP.\n",
        "\n",
        "    Pasos de preprocesamiento:\n",
        "    1.  Lee el archivo línea por línea.\n",
        "    2.  Elimina líneas que estén completamente en blanco o contengan solo espacios.\n",
        "    3.  Une las líneas restantes en un solo bloque de texto.\n",
        "    4.  Convierte todo el texto a minúsculas.\n",
        "    5.  Elimina signos de puntuación (reemplazándolos con espacios para evitar unir palabras).\n",
        "    6.  Elimina números (reemplazándolos con espacios).\n",
        "    7.  Elimina espacios en blanco extra (múltiples espacios, tabulaciones, etc.),\n",
        "        dejando solo un espacio entre palabras.\n",
        "    8.  Elimina espacios en blanco al principio y al final del texto resultante.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): La ruta al archivo .txt que se va a procesar.\n",
        "\n",
        "    Returns:\n",
        "        str: El texto preprocesado como una única cadena de texto.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: Si el archivo especificado en file_path no existe.\n",
        "        IOError: Si ocurre un error al leer el archivo.\n",
        "    \"\"\"\n",
        "    input_path = Path(file_path)\n",
        "    if not input_path.is_file():\n",
        "        raise FileNotFoundError(f\"El archivo no fue encontrado en: {file_path}\")\n",
        "\n",
        "    try:\n",
        "        #\n",
        "        with input_path.open('r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "    except IOError as e:\n",
        "        raise IOError(f\"Error al leer el archivo {file_path}: {e}\")\n",
        "\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        stripped_line = line.strip() # Elimina espacios/saltos al inicio/final de la línea\n",
        "        if stripped_line: # Si la línea no está vacía después de quitar espacios\n",
        "            cleaned_lines.append(stripped_line)\n",
        "\n",
        "    # Unir con un espacio para asegurar separación entre palabras de líneas adyacentes\n",
        "    full_text = ' '.join(cleaned_lines)\n",
        "    text = full_text.lower()\n",
        "\n",
        "\n",
        "\n",
        "    # Definir puntuación a eliminar!\n",
        "    # Podríamos querer agregar mas  símbolos si es necesario: '–', '—', ... string.punctuations puede ir tmb\n",
        "    punctuation_to_remove = string.punctuation + '¡¿«»“”‘’!?'\n",
        "    # Crear una tabla de traducción: cada carácter de puntuación se mapea a un espacio\n",
        "    translator = str.maketrans(punctuation_to_remove, ' ' * len(punctuation_to_remove))\n",
        "    text = text.translate(translator)\n",
        "    # Reemplazar secuencias de dígitos con un espacio\n",
        "    text = re.sub(r'\\d+', '00', text)\n",
        "    # print(text)\n",
        "    # Reemplazar una o más ocurrencias de espacios/tabs/newlines con un solo espacio\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B46SpUjWNseG",
        "outputId": "3211c9cc-b1d8-4a99-97be-efb6c45fb8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Procesando archivos y generando CoNLL-U...\n",
            "  Procesando noticia1.txt...\n",
            "    -> Guardado en noticia1.conllu\n",
            "  Procesando noticia2.txt...\n",
            "    -> Guardado en noticia2.conllu\n",
            "Proceso completado.\n"
          ]
        }
      ],
      "source": [
        "# Asegurarse de tener spacy, es_core_news_lg y spacy-conllu cargados en el entorno. Si no, ejecutar más arriba.)\n",
        "# Asegurarse tmb de que los archivos se llamen noticia1.txt y estésn cargados en el entorno.\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "nombres_archivos_txt = [\"noticia1.txt\", \"noticia2.txt\"]\n",
        "nombres_archivos_conllu = [\"noticia1.conllu\", \"noticia2.conllu\"]\n",
        "\n",
        "print(\"Procesando archivos y generando CoNLL-U...\")\n",
        "\n",
        "for txt_file, conllu_file in zip(nombres_archivos_txt, nombres_archivos_conllu):\n",
        "    print(f\"  Procesando {txt_file}...\")\n",
        "    try:\n",
        "        ruta_txt = Path(txt_file)\n",
        "        # texto_noticia = ruta_txt.read_text(encoding=\"utf-8\")\n",
        "        text_norm = preprocess_spanish_text(ruta_txt) # Lo mandamos a preprocesar\n",
        "        doc = nlp(text_norm)\n",
        "\n",
        "        conllu_output = doc._.conll_str # me quedo con el resultado de conllu\n",
        "        ruta_conllu = Path(conllu_file)\n",
        "        ruta_conllu.write_text(conllu_output, encoding=\"utf-8\") # lo guardo\n",
        "        print(f\"    -> Guardado en {conllu_file}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"    ERROR: Archivo {txt_file} no encontrado. Asegúrate de que exista.\")\n",
        "    except Exception as e:\n",
        "        print(f\"    ERROR procesando {txt_file}: {e}\")\n",
        "\n",
        "print(\"Proceso completado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJVxuv3ETzn3"
      },
      "outputs": [],
      "source": [
        "#!pip install conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjiYySldSpA8"
      },
      "outputs": [],
      "source": [
        "from conllu import parse_incr\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def analizar_conllu(filepath):\n",
        "    \"\"\"Lee un archivo CoNLL-U y extrae algunas estadísticas.\"\"\"\n",
        "    tokens_data = []\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for tokenlist in parse_incr(f):\n",
        "            tokens_data.extend(tokenlist) # Acumula tokens de todas las frases\n",
        "\n",
        "    num_tokens = len(tokens_data)\n",
        "    num_sentences = len(set(t['id'][0] for t in tokens_data if isinstance(t['id'], tuple))) # Aproximación contando sent_id si existe, o reinicios de ID=1\n",
        "    if not num_sentences: # Fallback si no hay metadatos o IDs son continuos\n",
        "         num_sentences = sum(1 for t in tokens_data if t['id'] == 1)\n",
        "\n",
        "\n",
        "    lemas = [t['lemma'].lower() for t in tokens_data if t['lemma'] not in stop_words_es]\n",
        "    pos_tags = [t['upos'] for t in tokens_data if t['lemma'] not in stop_words_es]\n",
        "    dep_rels = [t['deprel'] for t in tokens_data if t['lemma'] not in stop_words_es]\n",
        "\n",
        "    # Contar los 10 lemas más comunes (excluyendo puntuación)\n",
        "    lemas_comunes = Counter(l for l, p in zip(lemas, pos_tags) if p != 'PUNCT').most_common(10)\n",
        "    # Contar etiquetas POS\n",
        "    pos_counts = Counter(pos_tags)\n",
        "    # Contar relaciones de dependencia\n",
        "    dep_counts = Counter(dep_rels)\n",
        "\n",
        "    print(f\"\\n--- Análisis de: {filepath} ---\")\n",
        "    print(f\"  Frases (aprox): {num_sentences}\")\n",
        "    print(f\"  Tokens: {num_tokens}\")\n",
        "    if num_sentences > 0:\n",
        "         print(f\"  Tokens/Frase (promedio): {num_tokens/num_sentences:.2f}\")\n",
        "    print(f\"  Lemas más comunes: {lemas_comunes}\")\n",
        "    print(f\"  Distribución POS: {pos_counts}\")\n",
        "    # print(f\"  Distribución DepRel: {dep_counts}\") # Puede ser muy largo\n",
        "\n",
        "    # Podríamos devolver un diccionario o DataFrame para comparar fácilmente\n",
        "    return {\n",
        "        \"Archivo\": filepath,\n",
        "        \"Frases\": num_sentences,\n",
        "        \"Tokens\": num_tokens,\n",
        "        \"Lemas_Top10\": lemas_comunes,\n",
        "        \"POS_Counts\": pos_counts\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDUPYNuMS9V_",
        "outputId": "437de9eb-604e-42a2-cdd2-2863065bb315"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Análisis de: noticia1.conllu ---\n",
            "  Frases (aprox): 1\n",
            "  Tokens: 2282\n",
            "  Tokens/Frase (promedio): 2282.00\n",
            "  Lemas más comunes: [('00', 30), ('ser', 27), ('papa', 20), ('haber', 16), ('francisco', 15), ('vaticano', 11), ('último', 10), ('muerte', 10), ('capilla', 9), ('mundo', 8)]\n",
            "  Distribución POS: Counter({'NOUN': 479, 'VERB': 206, 'ADJ': 193, 'PROPN': 169, 'ADV': 62, 'AUX': 55, 'NUM': 39, 'SCONJ': 5, 'PRON': 5, 'ADP': 3, 'DET': 2, 'CCONJ': 1})\n",
            "\n",
            "--- Análisis de: noticia2.conllu ---\n",
            "  Frases (aprox): 1\n",
            "  Tokens: 727\n",
            "  Tokens/Frase (promedio): 727.00\n",
            "  Lemas más comunes: [('ser', 18), ('francisco', 7), ('sino', 5), ('construir', 4), ('mundo', 4), ('encuentro', 4), ('haber', 4), ('vivo', 3), ('enseñar', 3), ('vivir él', 3)]\n",
            "  Distribución POS: Counter({'NOUN': 156, 'VERB': 88, 'ADJ': 49, 'PROPN': 26, 'AUX': 25, 'ADV': 15, 'CCONJ': 5, 'PUNCT': 4, 'DET': 3, 'PRON': 2})\n",
            "\n",
            "Resumen para noticia1.conllu:\n",
            "  Frases: 1, Tokens: 2282\n",
            "  Lemas Top 10: [('00', 30), ('ser', 27), ('papa', 20), ('haber', 16), ('francisco', 15), ('vaticano', 11), ('último', 10), ('muerte', 10), ('capilla', 9), ('mundo', 8)]\n",
            "  POS Counts (Top 5): [('NOUN', 479), ('VERB', 206), ('ADJ', 193), ('PROPN', 169), ('ADV', 62)]\n",
            "\n",
            "Resumen para noticia2.conllu:\n",
            "  Frases: 1, Tokens: 727\n",
            "  Lemas Top 10: [('ser', 18), ('francisco', 7), ('sino', 5), ('construir', 4), ('mundo', 4), ('encuentro', 4), ('haber', 4), ('vivo', 3), ('enseñar', 3), ('vivir él', 3)]\n",
            "  POS Counts (Top 5): [('NOUN', 156), ('VERB', 88), ('ADJ', 49), ('PROPN', 26), ('AUX', 25)]\n"
          ]
        }
      ],
      "source": [
        "# --- Analizar los archivos generados ---\n",
        "resultados = []\n",
        "for conllu_file in nombres_archivos_conllu:\n",
        "     try:\n",
        "         resultados.append(analizar_conllu(conllu_file))\n",
        "     except FileNotFoundError:\n",
        "         print(f\"Archivo {conllu_file} no encontrado para análisis.\")\n",
        "     except Exception as e:\n",
        "         print(f\"Error analizando {conllu_file}: {e}\")\n",
        "\n",
        "#  Mostrar comparación en una tabla con Pandas\n",
        "if resultados:\n",
        "    df = pd.DataFrame(resultados)\n",
        "\n",
        "    # print(df)\n",
        "    for res in resultados:\n",
        "         print(f\"\\nResumen para {res['Archivo']}:\")\n",
        "         print(f\"  Frases: {res['Frases']}, Tokens: {res['Tokens']}\")\n",
        "         print(f\"  Lemas Top 10: {res['Lemas_Top10']}\")\n",
        "         print(f\"  POS Counts (Top 5): {Counter(res['POS_Counts']).most_common(5)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK4o0b79p5Dj",
        "outputId": "3ce22ef4-66fa-408a-bbde-bb192d16ca56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extrayendo lemas de archivos CONLLU...\n",
            "  - Extraídos 2282 lemas de noticia1.conllu\n",
            "  - Extraídos 723 lemas de noticia2.conllu\n",
            "\n",
            "Lemas listos para análisis TF-IDF.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def extract_lemmas_from_conllu(filepath: str) -> list[str]:\n",
        "    \"\"\"Lee un archivo CoNLL-U y devuelve una lista de lemas (lowercase, no puntuación).\"\"\"\n",
        "    lemmas = []\n",
        "    ruta_archivo = Path(filepath)\n",
        "    if not ruta_archivo.is_file():\n",
        "        print(f\"Advertencia: Archivo no encontrado {filepath}\")\n",
        "        return []\n",
        "    try:\n",
        "        with ruta_archivo.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            for tokenlist in parse_incr(f):\n",
        "                for token in tokenlist:\n",
        "                    # Usar .get si falta la columna\n",
        "                    lemma = token.get('lemma')\n",
        "                    upos = token.get('upos')\n",
        "                    # Añadir si el lema existe y no es puntuación, si no se saltea ese lema\n",
        "                    if lemma and upos != 'PUNCT':\n",
        "                        lemmas.append(lemma.lower())\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {filepath}: {e}\")\n",
        "        return []\n",
        "    return lemmas\n",
        "\n",
        "# Asumiendo que los archivos .conllu se llaman como antes!!\n",
        "nombres_archivos_conllu = [\"noticia1.conllu\", \"noticia2.conllu\"]\n",
        "lista_de_lemmas_por_noticia = []\n",
        "corpus_para_tfidf = []\n",
        "\n",
        "print(\"Extrayendo lemas de archivos CONLLU...\")\n",
        "for conllu_file in nombres_archivos_conllu:\n",
        "    lemmas_noticia = extract_lemmas_from_conllu(conllu_file)\n",
        "    if lemmas_noticia:\n",
        "        lista_de_lemmas_por_noticia.append(lemmas_noticia)\n",
        "        # Unir los lemas en un string para tfidfVectorizer\n",
        "        corpus_para_tfidf.append(\" \".join(lemmas_noticia))\n",
        "        print(f\"  - Extraídos {len(lemmas_noticia)} lemas de {conllu_file}\")\n",
        "    else:\n",
        "        print(f\"  - No se pudieron extraer lemas de {conllu_file}\")\n",
        "       # handlear la ausencia de lenmas\n",
        "        corpus_para_tfidf.append(\"\") # Añadir string vacío si falla\n",
        "\n",
        "# Verificar que tenemos datos para procesar\n",
        "if not any(corpus_para_tfidf):\n",
        "    print(\"\\nError: No se pudieron obtener lemas de ningún archivo. No se puede continuar con TF-IDF.\")\n",
        "elif len([c for c in corpus_para_tfidf if c]) < 2:\n",
        "     print(\"\\nAdvertencia: Se necesita al menos 2 documentos con contenido para calcular IDF. Los resultados pueden no ser significativos.\")\n",
        "     # Podrías optar por no ejecutar TF-IDF en este caso\n",
        "else:\n",
        "    print(\"\\nLemas listos para análisis TF-IDF.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxmmhCHpsxE8"
      },
      "outputs": [],
      "source": [
        "#nltk.download('stopwords', quiet=True) # Bajamos las stopwords de NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y145EzNn5RVO",
        "outputId": "fe7897b9-fcd8-48e1-e576-a49a6d894fe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculando TF-IDF...\n",
            "  Se usarán 313 stop words de NLTK.\n",
            "Matriz TF-IDF calculada: 2 documentos, 847 lemas únicos.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "\n",
        "tfidf_matrix = None\n",
        "feature_names = []\n",
        "vectorizer = None\n",
        "\n",
        "# Solo proceder si tenemos al menos un documento con contenido\n",
        "# (Idealmente, necesitamos al menos 2 para que el idf tenga sentido)\n",
        "if any(corpus_para_tfidf) and len([c for c in corpus_para_tfidf if c]) >= 1:\n",
        "    print(\"Calculando TF-IDF...\")\n",
        "\n",
        "    stop_words_es = stop_words_es = stopwords.words('spanish')\n",
        "    print(f\"  Se usarán {len(stop_words_es)} stop words de NLTK.\")\n",
        "        # También podemos agregar sw:\n",
        "        # stop_words_es.extend(['palabra_extra1', 'palabra_extra2'])\n",
        "    vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, norm='l2', stop_words=stop_words_es)\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus_para_tfidf)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    print(f\"Matriz TF-IDF calculada: {tfidf_matrix.shape[0]} documentos, {tfidf_matrix.shape[1]} lemas únicos.\")\n",
        "else:\n",
        "    print(\"No hay suficientes datos válidos para calcular TF-IDF.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6h1IwmsSkIB",
        "outputId": "44d7d0b3-e0be-4439-b51c-5a0fe677ca43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Lemas más Característicos por Noticia (según TF-IDF) ---\n",
            "\n",
            "Noticia: noticia1.conllu\n",
            "  - 00                   (Score: 0.4563)\n",
            "  - ser                  (Score: 0.2922)\n",
            "  - papa                 (Score: 0.2164)\n",
            "  - haber                (Score: 0.1732)\n",
            "  - vaticano             (Score: 0.1673)\n",
            "  - francisco            (Score: 0.1623)\n",
            "  - muerte               (Score: 0.1521)\n",
            "  - capilla              (Score: 0.1369)\n",
            "  - señor                (Score: 0.1217)\n",
            "  - último               (Score: 0.1082)\n",
            "  - santa                (Score: 0.1065)\n",
            "  - basílica             (Score: 0.1065)\n",
            "  - santo                (Score: 0.1065)\n",
            "  - comenzar             (Score: 0.1065)\n",
            "  - después              (Score: 0.1065)\n",
            "\n",
            "Noticia: noticia2.conllu\n",
            "  - ser                  (Score: 0.5085)\n",
            "  - francisco            (Score: 0.1978)\n",
            "  - encuentro            (Score: 0.1588)\n",
            "  - construir            (Score: 0.1588)\n",
            "  - sino                 (Score: 0.1413)\n",
            "  - cultura              (Score: 0.1191)\n",
            "  - diálogo              (Score: 0.1191)\n",
            "  - mismo                (Score: 0.1191)\n",
            "  - vos                  (Score: 0.1191)\n",
            "  - continuar            (Score: 0.1191)\n",
            "  - haber                (Score: 0.1130)\n",
            "  - vivir                (Score: 0.1130)\n",
            "  - mundo                (Score: 0.1130)\n",
            "  - enseñar              (Score: 0.0848)\n",
            "  - hoy                  (Score: 0.0848)\n"
          ]
        }
      ],
      "source": [
        "def get_top_tfidf_lemmas(doc_index, tfidf_matrix, feature_names, top_n=10):\n",
        "    \"\"\"Obtiene los top N lemas para un documento específico.\"\"\"\n",
        "    # Asegurarse de que la matriz y los nombres existan\n",
        "    if tfidf_matrix is None or not feature_names.any():\n",
        "         return []\n",
        "\n",
        "    doc_scores = tfidf_matrix[doc_index].toarray().flatten()\n",
        "    # Obtenemos índices ordenados de mayor a menor score\n",
        "    sorted_indices = np.argsort(doc_scores)[::-1]\n",
        "    # Seleccionar los top N índices (asegurarse de no exceder el número de features)\n",
        "    top_indices = sorted_indices[:min(top_n, len(feature_names))]\n",
        "    # Obtengo tmb  los lemas y scores correspondientes\n",
        "    top_lemmas_scores = [(feature_names[i], doc_scores[i]) for i in top_indices if doc_scores[i] > 0] # Solo incluir si score > 0\n",
        "    return top_lemmas_scores\n",
        "\n",
        "# Mostrar los resultados si se calculó TF-IDF.\n",
        "if tfidf_matrix is not None:\n",
        "    print(\"\\n--- Lemas más Característicos por Noticia (según TF-IDF) ---\")\n",
        "    for i, filepath in enumerate(nombres_archivos_conllu):\n",
        "        # Solo mostrar si el documento original tenía contenido\n",
        "        if corpus_para_tfidf[i]:\n",
        "            print(f\"\\nNoticia: {filepath}\")\n",
        "            top_lemmas = get_top_tfidf_lemmas(i, tfidf_matrix, feature_names, top_n=15) # Pedir 15, por ejemplo\n",
        "            if top_lemmas:\n",
        "                for lemma, score in top_lemmas:\n",
        "                    print(f\"  - {lemma:<20} (Score: {score:.4f})\")\n",
        "            else:\n",
        "                print(\"  No se encontraron lemas con score TF-IDF > 0.\")\n",
        "        else:\n",
        "             print(f\"\\nNoticia: {filepath} (Archivo vacío o no procesado, se omite)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnSpZ-ezrz6u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
